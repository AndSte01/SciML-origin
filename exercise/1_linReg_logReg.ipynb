{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnxmQiHjifYTwuuYtT8ilK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturtoshev/SciML22-23/blob/master/exercise/1_linReg_logReg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Linear regression and logistic regression"
      ],
      "metadata": {
        "id": "ZAikSL9pqSHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial shows how to apply linear regression and logistic regression using PyTorch. "
      ],
      "metadata": {
        "id": "2raUSLK3qlWM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRemFotxqQJf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# visualization libraries\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# pytorch dependencies\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "# import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Linear Regression"
      ],
      "metadata": {
        "id": "mNu-XE50r0ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem setting\n",
        "- Given: given is a set of measurement pairs $\\left\\{x^{(i)}, y^{\\text {(i)}}\\right\\}_{i=1,...m}$ with $x \\in \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}$\n",
        "- Question:  if a give you a novel $x$, what would be your best guess about its corresponding $y$?\n",
        "- Linear regression assumption: \n",
        " $$y \\approx h(x)=\\vartheta^{\\top} x = \\vartheta_0 + \\vartheta_1 \\cdot x_1 + ... \\vartheta_n \\cdot x_n$$\n",
        "\n",
        "> Note: $\\vartheta_0$ is the so called bias term and you can read more about it in the \"Analytical Solution\" subsection below.\n",
        "\n",
        "<br>\n",
        "\n",
        " In matrix form this becomes:\n",
        " $$Y \\approx X \\vartheta$$\n",
        "\n",
        " <br> \n",
        "\n",
        " As a reminder, we can interpret the linear dependence assumption as a Maximum Likelihood Estimation of the \"true\" underlying linear dependence between inputs $x$ and outputs $y$ with added Gaussian noise on top:\n",
        "\n",
        " $$Y = X \\vartheta + \\epsilon, \\quad \\text{with} \\; \\epsilon \\sim \\mathcal{N}(0,\\sigma^2), \\; \\epsilon \\in \\mathbb{R}^n$$"
      ],
      "metadata": {
        "id": "kkOEHPD1r4lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Artificial Dataset"
      ],
      "metadata": {
        "id": "Q_22ET7zxVrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 3\n",
        "b = 2\n",
        "x = np.random.rand(256)\n",
        "\n",
        "noise = np.random.randn(256) / 4\n",
        "\n",
        "y = a + b*x + noise"
      ],
      "metadata": {
        "id": "fSYPeIHIxjD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, we use Pandas to store our values in the dataframe and then access them out of the dataframe - this is a highly common workflow for machine learning datasets. A normal split would be a Pandas dataframe for labels, serial data etc. and images in the same folder, which can then be described with the PyTorch DataSet API."
      ],
      "metadata": {
        "id": "penQflaWxgJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df['x'] = x\n",
        "df['y'] = y\n",
        "\n",
        "plt.scatter(df['x'], df['y'], color='green')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "kmVv9ecGr3r8",
        "outputId": "60de1d9a-7b2a-4fa8-dfe7-dfcf98477f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Gradient Descent Optimization"
      ],
      "metadata": {
        "id": "9ijT-0JC3Ecp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lecture we saw that for an iterative optimization process, e.g. gradient descent, we need to define a measure $J$, which capture the error. This quantity is what we essentially minimize through repeated updates of the parameters $\\vartheta$. One very common **error function** $J$, a.k.a. **loss** or as PyTorch calls it **criterion**, is the mean squared error (MSE), a.k.a. squared L2 loss:\n",
        "\n",
        "$$J(\\vartheta)=\\frac{1}{2} \\sum_{i=1}^{m}\\left(h\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$$"
      ],
      "metadata": {
        "id": "hwH5sZd73mvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the input variables\n",
        "x_train = x.reshape(-1, 1).astype('float32')\n",
        "y_train = y.reshape(-1, 1).astype('float32')\n",
        "\n",
        "# Definition of the linear regression model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "model = LinearRegressionModel(input_dim, output_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "pexfhkMx3IUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the performance of the model **before** we do any optimization:"
      ],
      "metadata": {
        "id": "JwztZXus5SSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data')\n",
        "plt.plot(x_train, predicted, '--', label='Predictions')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2_txT-gl5aVO",
        "outputId": "476a7137-10fd-4859-e1e3-0199f2975833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is currently initialized with some random numbers. These are:\n"
      ],
      "metadata": {
        "id": "rQ1yUnnW_QVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, \": \", param.data)\n",
        "  # bias = \\vartheta_0\n",
        "  # weight = \\vartheta_1:n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViN7MmVa9rXf",
        "outputId": "8bbb1323-a271-4983-ab71-72dba74dc640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight :  tensor([[0.2690]])\n",
            "linear.bias :  tensor([0.0701])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    inputs = Variable(torch.from_numpy(x_train))\n",
        "    y_train_var = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Output from model given the inputs\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Get loss for the model's prediction\n",
        "    loss = criterion(y_pred, y_train_var)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model's parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Exjkcqz3Oxp",
        "outputId": "a00db37d-3efd-4786-d095-58b98830fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 14.930853843688965\n",
            "epoch 1, loss 14.180635452270508\n",
            "epoch 2, loss 13.468267440795898\n",
            "epoch 3, loss 12.791839599609375\n",
            "epoch 4, loss 12.149541854858398\n",
            "epoch 5, loss 11.539649963378906\n",
            "epoch 6, loss 10.960528373718262\n",
            "epoch 7, loss 10.410627365112305\n",
            "epoch 8, loss 9.888469696044922\n",
            "epoch 9, loss 9.392656326293945\n",
            "epoch 10, loss 8.921858787536621\n",
            "epoch 11, loss 8.474814414978027\n",
            "epoch 12, loss 8.050324440002441\n",
            "epoch 13, loss 7.647252559661865\n",
            "epoch 14, loss 7.264516830444336\n",
            "epoch 15, loss 6.901090621948242\n",
            "epoch 16, loss 6.556001663208008\n",
            "epoch 17, loss 6.228323459625244\n",
            "epoch 18, loss 5.917176723480225\n",
            "epoch 19, loss 5.621729373931885\n",
            "epoch 20, loss 5.341187477111816\n",
            "epoch 21, loss 5.074800491333008\n",
            "epoch 22, loss 4.8218536376953125\n",
            "epoch 23, loss 4.581668376922607\n",
            "epoch 24, loss 4.353601455688477\n",
            "epoch 25, loss 4.1370415687561035\n",
            "epoch 26, loss 3.9314072132110596\n",
            "epoch 27, loss 3.736147880554199\n",
            "epoch 28, loss 3.5507407188415527\n",
            "epoch 29, loss 3.374687433242798\n",
            "epoch 30, loss 3.207516670227051\n",
            "epoch 31, loss 3.0487802028656006\n",
            "epoch 32, loss 2.89805269241333\n",
            "epoch 33, loss 2.754929542541504\n",
            "epoch 34, loss 2.619027614593506\n",
            "epoch 35, loss 2.4899823665618896\n",
            "epoch 36, loss 2.3674476146698\n",
            "epoch 37, loss 2.2510955333709717\n",
            "epoch 38, loss 2.140613555908203\n",
            "epoch 39, loss 2.03570556640625\n",
            "epoch 40, loss 1.9360904693603516\n",
            "epoch 41, loss 1.8415015935897827\n",
            "epoch 42, loss 1.7516846656799316\n",
            "epoch 43, loss 1.666399359703064\n",
            "epoch 44, loss 1.5854167938232422\n",
            "epoch 45, loss 1.5085203647613525\n",
            "epoch 46, loss 1.4355030059814453\n",
            "epoch 47, loss 1.366170048713684\n",
            "epoch 48, loss 1.3003346920013428\n",
            "epoch 49, loss 1.2378214597702026\n",
            "epoch 50, loss 1.178462028503418\n",
            "epoch 51, loss 1.122097373008728\n",
            "epoch 52, loss 1.0685763359069824\n",
            "epoch 53, loss 1.0177556276321411\n",
            "epoch 54, loss 0.9694989919662476\n",
            "epoch 55, loss 0.9236771464347839\n",
            "epoch 56, loss 0.8801666498184204\n",
            "epoch 57, loss 0.8388514518737793\n",
            "epoch 58, loss 0.7996208071708679\n",
            "epoch 59, loss 0.7623693346977234\n",
            "epoch 60, loss 0.7269974946975708\n",
            "epoch 61, loss 0.6934099197387695\n",
            "epoch 62, loss 0.6615172624588013\n",
            "epoch 63, loss 0.6312332153320312\n",
            "epoch 64, loss 0.6024772524833679\n",
            "epoch 65, loss 0.5751718878746033\n",
            "epoch 66, loss 0.549244225025177\n",
            "epoch 67, loss 0.5246245861053467\n",
            "epoch 68, loss 0.5012471675872803\n",
            "epoch 69, loss 0.4790492653846741\n",
            "epoch 70, loss 0.45797106623649597\n",
            "epoch 71, loss 0.43795645236968994\n",
            "epoch 72, loss 0.41895145177841187\n",
            "epoch 73, loss 0.40090519189834595\n",
            "epoch 74, loss 0.3837693929672241\n",
            "epoch 75, loss 0.36749809980392456\n",
            "epoch 76, loss 0.35204780101776123\n",
            "epoch 77, loss 0.33737674355506897\n",
            "epoch 78, loss 0.3234459161758423\n",
            "epoch 79, loss 0.3102179765701294\n",
            "epoch 80, loss 0.2976572811603546\n",
            "epoch 81, loss 0.28573012351989746\n",
            "epoch 82, loss 0.2744048237800598\n",
            "epoch 83, loss 0.26365089416503906\n",
            "epoch 84, loss 0.25343942642211914\n",
            "epoch 85, loss 0.2437431663274765\n",
            "epoch 86, loss 0.23453602194786072\n",
            "epoch 87, loss 0.2257932722568512\n",
            "epoch 88, loss 0.2174915373325348\n",
            "epoch 89, loss 0.20960867404937744\n",
            "epoch 90, loss 0.20212343335151672\n",
            "epoch 91, loss 0.1950157731771469\n",
            "epoch 92, loss 0.18826670944690704\n",
            "epoch 93, loss 0.181858092546463\n",
            "epoch 94, loss 0.17577263712882996\n",
            "epoch 95, loss 0.1699942797422409\n",
            "epoch 96, loss 0.16450728476047516\n",
            "epoch 97, loss 0.15929710865020752\n",
            "epoch 98, loss 0.154349684715271\n",
            "epoch 99, loss 0.14965187013149261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data')\n",
        "plt.plot(x_train, predicted, '--', label='Predictions')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  print(name, \": \", param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "OSVTSGXJ_fyo",
        "outputId": "19be0077-f7de-4804-c438-47708871b394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de3hU1dX/PztXAoEgQYKiSVDBoka8pF5KfxourUBQq69VbMiLIEXUt0UtbbV56z2VKlVpvVBKVUpjqfVFRUGtAalKRY1VRLwASojXIEEigZDb7N8fkxkmk3Nmzpk5J3PJ+jyPj5lz3XsSvnvttddaW2mtEQRBEBKflFg3QBAEQXAGEXRBEIQkQQRdEAQhSRBBFwRBSBJE0AVBEJKEtFi9ePDgwbqwsND2ffv27aNfv37ONyiOkT73DqTPvYdo+v3mm2/u0lofanQuZoJeWFhITU2N7fvWrVtHSUmJ8w2KY6TPvQPpc+8hmn4rpXaYnROXiyAIQpIggi4IgpAkiKALgiAkCSLogiAISYIIuiAIQpIggi4IguAgVZuqKLy3kJRbUii8t5CqTVXdjk/dMNV/3EliFrYoCIKQbFRtqmL207PZ37YfgB2NO5j99GzW161n6cal/uP1LfXMfno2AGVFZY69Xyx0QRAEh6hYU+EXbR/72/az+M3Fhscr1lQ4+n4RdEEQBIeoa6wzPN6hO2xdHyki6IIgCA6Rn5NveDxVpdq6PlJE0AVBEByicnwlfdP7djnWN70vs0+dbXi8cnylo+8XQRcEQXCIsqIyFp+7mIKcAhSKgpwCpo+ezuqtq9nftt9vqedl5rH43MWOLoiCRLkIgiA4SllRmV+og6NeOnQHfdP7Mmv4LMfFHMRCFwQhCsxirgUvZlEvS7YvceV9IuiCIESEz/rc0bgDjfbHXCe7qNsZxMyiWHa27HSlbSLogiBEhJn16XRsdTxhdxAzi2LRaFdmNCLogiBEhJn16XRstVtE4i6yO4gZRb34cGNGI4IuCEJEmFmfTsdWu0Gk7iK7g1hg1IsRTs9oRNAFQYgIs5hrp2Or3SBSd1Ekg1hZUVnI78TJGY0IuiAIEWEUc+1GbLUbROouinQQCzVQODmjEUEXBCFiyorKqL2mFs9NHmqvqU0IMYfI3UXBg1huVi5ZaVmUrygP6YcPNVA4OaMRQRcEodcRjbvIN4gtu3AZze3NNDQ3hPXDmw0UuVm5Uj5XEAQhGpxwF9nxw5sNIAsnLYysAyZI6r8gCL2SwBT9SLDjh/e9p2JNBXWNdQzJHMLvSn8ntVwEQRDigfycfHY07jA8bkTgALJu3TpKikocb5O4XARBECIgHsM2RdAFQRAiIB7DNkXQBUHoQjQVFJO1+qJZv+ItbFN86IIg+DHbtR7C704fzb3xTCL1Syx0QRD8RFNBMVmrLyZSv0TQBUHwE00FxUSvvmhGIvVLBF0QBD/RVFDsqeqLPe2nT6SqkpYEXSlVq5TapJR6WylVY3BeKaV+r5TappR6Ryl1ivNNFQTBbaIJxeuJMD47ZW+dEv54DE80w46FPlZrfZLWutjg3CRgROd/s4EHnWicIAg9SzSheD0RxmfVn+3k9njxGJ5ohlNRLucDf9Faa2CDUmqgUuowrfUXDj1fEIQeIpqU+GjT6cNh1Z8dSvgjaV8k/araVOVP9c/PyadyfKXrg4DyanCYi5TaDnwNaOCPWuvFQeefAeZrrV/p/LwG+KXWuiboutl4LXjy8vJOXb58ue0GNzU1kZ2dbfu+REb63DuQPodn6oap1LfUdzuel5nH8jMO6sm4f41D013bFIq1Z6+NrLFAdX01S7YvYWfLToZkDuGMQWewYfcG/+dZw2cxIW8C1fXVLNiygBZPi//ezJRM5o2cx4S8CVH9rseOHfumiafEsqAP01p/ppQaArwA/ERr/VLAeUuCHkhxcbGuqTE9bcq6desoKSmxfV8iI33uHUifwxMcEw5ef3awC6Tw3kLDOisFOQXUXlNr+V2BFvbkEZNZunFpN8s/EF9bKtZUhHx/NL9rpZSpoFvyoWutP+v8/07gCeC0oEs+A44M+HxE5zFBEATHsOrPjnYh08gHv6hmUUgxh4NunViFOoYVdKVUP6VUf9/PwPeBd4MuWwn8d2e0yxlAo/jPBUEA58MMraTbR7uQaeSDN3LhGOGz6I1wO9TRyqJoHvCEUsp3/aNa6+eUUnMAtNaLgNXAZGAbsB+Y4U5zBUFwgp5asDNLm19ft57VW1d73/+2O++PZoE2Gkva930auYbcDnUMK+ha64+B0QbHFwX8rIGrnW2aIAhu0JO1ScyiTRbVLPJbvPFYG8Ws1rlChbTUfaIdvKFFT0W5SKaoIPQyerI2iZmlGyyK8VYbxcwHP6d4Thc3zpXFV5q6dWJRiVGqLQpCL6MnF+zMLN2een+k2LGwA91XvkEpVjMNsdAFoZfRkwt2RpauQll+fyzrq1uxsJ3MSHUCEXRB6GX0ZG0So2iTOcVzLL0/3sTSiHgrrSuCLgi9jJ6uTRJs6T5Q+oCl99sVSzNr3k0r38ydZNXN5DTiQxeEXojbNVesvj9UxqQdX79R5E75inIefuthXv301dBhk1FEoKSqVDp0h+HxWCAWuiAIlrFq7TphFdvx9ZslAq3ZvsY0bDLQlVO+opyrVl1lu41GYh7quNuIoAuCYAmrPm2nfN92fP12I2SCwyY1mgdrHkTdomwNQAU5BbaOu40IuiAIlrDq03ZqoTDQ1w9eN4bvOcGC62SEjp0ByGzQmTxickyic0TQBUGwhFWfth3fd3V9dUjhKysq84umz41hJLiV4ytNwyGDj5tdF4jZphnBbTVaYJ4+ejpLNy6NSXSOCLogCJYI5dMOFLsUZSwrwfdXbapiwZYFYYXPzOKf/sR0v7gCzCme002sjbI7xw0fZ6m/gQNQKDdScBTP6q2rYxbKKIIuCIIlQrkXAsXOaEHQyPddsaaiywYQYCx8ZiGAHbqji7iOyR/DsguXdQuHfKD0Ab/gVo6v5NVPX7XU38AByI4bKValc0HCFgVBsIhZOryR2IHX5+3RHtOwQCvCV7WpKmxBLDgoruFqppi1NZjgAciOSJuVO3C7dC6IoAuCYAOj+PXyFeWG13q0B89NHtNnWRG+ijUVtuqQO3FNqkrtluhkR6RjVToXxOUiCEKUWI0XD15UnDxiMql0T8DZ0bjDv+hox01hxQIOd41C0aE7ukXS2Amh7OlM3EBE0AWhF+FGGrwVsTNaVFzynyWmz/T5xQdlDTI8b7T4acUCNmprIME12n3fj12RjkXpXBBBF4SY4xPZcf8a52rMspGoRpohGYgVsTPyXbd52ujAPKPSd32ouuQQOj49VFshdAhj8KJnrETaDiLoghBD3K4oGGiRT39iumF6/KKaRa7v8xlphMfu5t2Gg8UDpQ9Yik8P1daCnIKw/vl4qtFuBRF0QYghbpZfDR4szOqLaLTrMdKRRnjk5+SbDhbRfndWxLonIlOcRARdEGKImzHLVkP0nHqfGVWbqmhqbbJ9X3pKeki/eLTfXTixDvf+eEQEXRBiiJu7BzkdIRIJvllCQ3OD7XuVCp2iH+13Z3WBNJEQQReEGOLm7kFWhc3NGGk7s4RgWjtaQ7pPov3ughdIg2n3tDP32bmWnhXLrfICEUEXhBjiZsyymeCF2qneKXwCF+3OPaFmGU58dz7/vBlWZhZVm6qY8eSMLgvbM56c0UXU2zo8zPvHRu587gPLbYsEyRQVhBhjZfeeSJ8L1naud5Lg3YOMyM3KJTsj29+uptYmQ/EMN8uI9c5LAHOfnUubp63LsTZPG3OfncvE4Rdx6Z82sKX+4BrCdd8b6VpbRNAFIYCqTVU9LoBu0hOCF/ydNbU2hRTzvul9WThpYZdBzGgQ6Kl0efAOMEYDSm5Wbth7Da14DRlf38ipt1f7D5UWHcbdl4wmLdU9x4i4XAShk0TYZd4It/y3vueqWxRpt6YZ7uZj9J2FclOYuUUicZ842e+FkxaSnpLe5Vh6SjoLJy209Zw+HcWke47Em6/ktZdvmPQttt8xmfvLTiEzzd29RsVCF4ROQsU1x6uVbrQ58uynZwNE1ebg5wYn7/ieb2fRsyCnIKS/2s5swul+R+Oeyu0zGM/e8xnQfgEALWoLX/a5jtaBN/P5L3fabks0iIUuCJ3Eso51pLiVmBRKqAOfb/W7cdp94ka/7ab2t7R3MOPh18n++hG/mHvYz66M35KRmsHCyfdE3JZIEUEXhE7cjAl3C7cGoXD3+85b+W58tVamPzEddYti8J2DGXznYFJuSWHqhqkRuUpiOfh+va+VDo+mcX8bL374FQBH5bXBkOv4NOsShh3Sh4fOf8g/IPRkSKNlQVdKpSql3lJKPWNw7jKl1FdKqbc7/5vlbDMFwX3ibcNfK0QyCFkRmHBC7TsfLjnHV44WDrptGpobaGhuQKOpb6mPaJ0iFoPvix/upPD6VZx82wu88F49Qwb0YdVPv8vHv5nM2mt/QO11H3az7s3WZarrq8O8LTLsWOhzgfdDnP+71vqkzv/M62IKQpwSbxv+WsFuck24hd/A+HGzSoSBz/d9Z0bRIFZ2GgJzV0mogcfNhKxAtNbc/+I2Cq9fxYyH3/AfHz64HwDHH55DSop5RquZa2jJdnck0tKiqFLqCKAUqASuc6UlghAHBC/MFd5bGNcLpXYX88L5ngMXGjXaL8qpKpUO3UFBToFfNAvvLfS/0xcNEtgOO0lFwa6ScIueTsfYm4WrrvjPZ9z1/If+61b99Lscf3hOxP3ysbPFncVSpXX4EVQp9ThwB9AfmKe1nhJ0/rLO818BW4BrtdafGDxnNjAbIC8v79Tly5fbbnBTUxPZ2dm270tkpM+xY9y/xhlamQrF2rPXOvoup/pcXV/Nku1L2NmykyGZQ5g1fBYT8iYAofszJHMI9S313c7lZeax/IyD/1ar66tZsGVBlw2eM1MymTdynv89AFM3TDV8nhHB7zC7N/g6JwjsT6rOZWjLXaTpIUwd/RpnHzqWx7e08oNjMuifobrcY/YdB2LWj0MzDuWxMx+LqL1jx459U2tdbHQurKArpaYAk7XWVymlSjAW9FygSWvdopS6ArhEaz0u1HOLi4t1TU2NnX4AOJ5NlwhIn2OHWfp6uBC8SHCiz2YJOr6Y7lD9qWusM3WRKFSXTaGtfCdWMkaD2+cj5ZYU04En1D6lkVB4byFf7s5iaOuCricG30jtvDe7XR/uO7Zy7bVHX8vtl9weUXuVUqaCbsWHPgY4TylVCywHximl/hp4gda6QWvtG66XAKdG1FJBiDN6ylfrFOFcKpXjKw0TaCrHV4ZcUAz0t5u5UoLdC8HFr1KVN6kmNyuX3KxcFIq8zDxDIezJRc99X13cRcwb0h9gR9YU6va9ZXi9nZBJs4QpI2veCcL60LXWNwA3AARY6NMCr1FKHaa1/qLz43mEXjwVhIQhFvVQoik/YCWcL7gsre+z0W71wexv2+/3pwdjJLbhkoXWrVtHSVFJt+NGbXFqIG3v8PDrpzZzeE4ffjJ+BP36b4E9Z/Nlxi9pSd3sv85s8LAbMmn0Haxbty6yxoch4kxRpdStQI3WeiXwU6XUeUA7sBu4zJnmCULs6ckCUNFmQJotRvrEqWJNBa0drV3O+crU+twlvsHEzP3SoTvom97X1borbgyke/a3UrbkNTZ//o3/2FVjj6Gy9IfMfvpiWiz2J9x3HEtsJRZprdf5/Oda6xs7xRyt9Q1a6+O11qO11mO11u7WiBSEJCXaDMhwLqJw1mVgtqRZnXCf28DtErxObcrs8WhKf/8yJ936gl/Mv3dcHh/cNpHUFGW7jkw8u+Gklosg9ABW3SjRZkCGs2ztWJeh3B5WZy2xrF65ftsuhub04ehDs9mz31ve9mffG8n/jDumm9vJziwsVmWJrSCCLgguU7WpiplPzfS7OnY07mDmUzOB7m4UJ6bzocRp8ojJPFjzoOFxo+dA5MLlVuGwUGituev5D3lg3UcAFBccwuNXfod1Py8h3cGytfFQh90IEXRBcJm5z8419FvPfXZuN1FwczEQYPXW1baORyNcPVm9srXdw1VVb1L9/sGEnfRUxV0/HN35c+8oW9U7eikIMcSsPrjRcTe3pAP3i1oFputbDW+Mhsb9bXg8ml1NLX4xP7XgEN65+ftsrZzsT9GPlz0/3UYsdEGIM9yczrsVoVG1qYq5z861tAdn8Lsi8bO/snUX0/78GgB/nl7M+FF5PHn1GE4c1r22SixcP7FCLHRBcBmzbcxys3K7WI6RlpK1gxsRGj7BtCLmwe+qrq+2tUvUkpc/pvD6VX4xBzjiEG9/TjpyoGGhLLdqxscjIuiCK8RiittT77T7HqPtzcDrcilfUe4Xs0hLydrBqDpiVlpWVM+0smuRmftoyfYllsX2sTc+4fZVB3MWV/7PGGrnl3Ls0P4h352IG5dEigi64Dix2JvT7jsjFf9I+lZWVMbDP3jYH9cdWJY2OHnHquUY7eDV3N7s/7mhuSGq3084YSzIKTCNJTerOljXWMfOvQcouetFCq9fRUt7B+ecMJSp3z6SNyomUDu/lBOPGGipfYm4cUmkiKALjhNqiuuWFW1nWh3NgBPp9N2XJFOQUxC2Rng4gYx2wHTaBRFKGMO5c4ZkDul2LMMzgvzmpzmtcg21Dd527vymhZysdOb/14kc2j/TVvviORHIaUTQBccxE6TA4k6RWNFXrbrKdDCwM622K2hORm5YuS6c5RitIDvtgjDbtSg3KzdshM6s4bO63Du49Rcc1nJwL85fTzmO2vmlHDnIfFekcLgdORRPSJSL4DhmkRS+vSUDMYtLrq6v5p5/39MlMiEwISY4UsFO9IYdQbNaAtbq9D3cxg9WLMdoBdmsDSkqhapNVbaFLpoEpHFDxrNt3yls+GI1de0P0S97GzSeRdWs0xlzzGBb7QjXxmQU8GDEQhccx2yKa1ShD4yFyGixLJjgsrBWp9V2fKpWFvzsTN+N2unzqZuVkrXSzlDHg2c6k0dMNrSoO3RHxL50o7orodxrjc1tXPDAemY+v58NWzJh7wW0/bqD2hv+j9r5paZi3lviySNFBF1wHLMprlmxJyMhsrpFV2BRKavTajviH8rqjWT6btTOZRcuQ9+kWX7GckvPsdN+I3/70o1LmT56ur8+eSBOhfOZ+fmXvVPFuX94hdG3/JO36vYAcPbIQ3nv1nNIDbE3Z6hniqgfRFwugiuYTXGtprWbbYcWTOBgYHVabcdFYOaeiGbHomin/3bab+Zvf2zzY7ZmTHYJfm9mx3G0enbz67UVDNr7JwB+Mu4YTkn/nLFjT4vomRBf+7vGA2KhCz2GHSs6eLHMiGgiFayWZu2JCAmfG2Hcv8ZZdiNYbb+ZOIdKArITzmfmAqlrrAMNOW2XUtD8DENb72RQ29XsaNzB2x2lMPRqhh5W063qYSh6Uzx5pIiFLvQoVq3TCXkTGHXcqC5W6OQRk1m9dXWPlix1slSqUYo74GpaerhF2GDsDFZmKfUdHsj33AItp3S5fnf6IgC0avNfe+3R11JCiaX0/3jeWCJeEEEX4pZ4iUxwoh1m4peVlmXoRpj+xHTW162PegCzsq1cIHbWA4JdIEr3ZX/rfm584XfQchsALWoLOzN/jUft63b//rb9LNm+hFGbRlka1NyuRJkMiMtFcBS3ohDiIbrBShvMrjHz/5q5Pjp0Bw/WPBj1AqCRm8ustkxBToGtAcPn6sjsOIGC5mfIP/AYfTwnU7fvbR674kxuu3QPmXkL0cp8MNnZstNyXH1viiePFLHQBcdwo6qdURW/WFTLs9I3o2vKV5QzbcU044faJNIFwOAZhlFsfSSWbn5aOey9uMuxDrWb/Jx8Ths+iNMoo3y0972F9xYaukuGZA6x5RuPl1lbvCIWuuAYTqeUh6ri19PV8qz0zeiacGn+uVm5YRd/Awm1AGh1FuOEpfu31+u6iPkXmT9jR9YU0jO/MhwYzBaXZw2f1atqrbiNWOiCYzgdhRAuqcfN6IbgRTorKf9229M3vS8LJy0EYPoT003DCAMJlTxkZ3Zk19JtaGrh0j9tYEt9E1tun8TE44fy+vbdjCx8j9+++ivaGusoyCkw9fObLS4PaxjGqONGiW/cIUTQBcdwOgohnEC6ZcEZiaNCGVrbgW2wGlGiUIaLnOFcM6FEzq0Y7Xc/a2TKH17pcuyrphaGDczinktOAk7iytN/ZOlZRoPIunXr4nrT5URDBF1wDKejEEIJpJsWnJnrJFjUg9tgJaLELCGprKiMWU/M4oA+YHpfKJFzI0b7qqo3Wb3pS//nGyZ9i9lnHWUrdtwq4ht3BvGhC44RrW82ePces5ojVqr4RYOZCGq0Yd987S5fUU5WWhbZGdmG96enpIcchFp0i+FxhTJMHgr8vlKU8T/l/Jx8y751j0dTueo9Hlm/HYCxx3pL2y6deRq180u54uyjXRFzwTnEQhccJVJLK9jNUd9S7685YjUWO5K9KY2wk+4f3O6G5oYuG1gEMiBzQMj2mJU7MHItBb/XyP/eN70vk0dMDutb33ugjcsfqeH12t3+e//7zEJ+WHwkPyw+0rS9QvwhFroQF5j5gFdvXW0pxd3Jwk120v3tRLbsbt5teNyHUbkDO+8Fb4niwBnE6q2rTX3rHR7N+fe9QtHN//SL+ZlH5fLuLecY7s0pxD9ioQtxQbQ+YCcXBe0s0tnxUYdbxDUqd2D3vR7twXOTx/+5fEV5t2syPCP57OtGUlMUH9bvBeCKs4/il+d8S4Q8wRFBF+KCaCNkohkQzFw1VgYCs3aHW0A1I9r3Bn9f/us0DGj/IYe0T/eeyNwEwNs3fp8+6d3L6AqJibhchLjAbOOHySMmW7o/0uSUaF01leMryUjN6Ha8b3pfcrNyXUtRt+oWunVsJXltN1Bw4JmDYg7M/Z63Nr2IeXJhWdCVUqlKqbeUUs8YnMtUSv1dKbVNKfWaUqrQyUYKsaEn66eUFZUxffT0LguKGs3SjUstvddumVtf36atmBZ1dqvW3X3m+9r20dzezLILl4X0/UdKuIiippZ2tNacNewH9GkfA0Cr2g5DfkFl2R6u/e6ljrZHiA/suFzmAu8DAwzOXQ58rbU+Rik1FfgtcIkD7RNihBt1WcKxeuvqbguKVv3gdvzeVvYJteO7b/O0GZ7b37afaSumUbGmwpVEGSP3TE3tbi5a9CoAj/74dL5z9GCqZp3O6cMHkZaaAvyPo20Q4gtLFrpS6gigFFhicsn5wNLOnx8HxisJWE1onK7LYoVoF0atbvpgZZ9QjbY0K7HStlBunODY+0hnQY++Vkfh9av8Yg4wqJ/XFTTmmMGdYu4+8VAVszejjKaL3S5S6nHgDqA/ME9rPSXo/LvARK31p52fPwJO11rvCrpuNjAbIC8v79Tly5fbbnBTUxPZ2caJG8lKLPo87l/jDMPvFIq1Z6915Z1TN0w1jMPOy8xj+Rn2/1bMMOubEZkpmcwbOY8JeRMMz5u12YgBqQPISstiZ8tOhmQO4YxBZ/Bc/XO0eA4mFIV7nxFr69r4y3ut/s83nNaHYwf1vG+8ur6aBVsW2OpPb/z3DNH1e+zYsW9qrYuNzoUVdKXUFGCy1voqpVQJUQh6IMXFxbqmpsZeT/DWfigpKbF9XzwTLiEmFn02K3cazV6a4TAr6+r0gqJZ38wI1Wcr7hszzOrDhPuO9+xvpWzJa3zw5V4+vG0ie5rbuOmpzVSUjuLwgVm22+EUkfzNJOO/ZytE02+llKmgW5mHjQHOU0rVAsuBcUqpvwZd8xlwZOfL0oAcwHzTQsFPvO5k3hN7aQbTUxsYmPXNjFBuFV+bU5V9i9hslmD2vg++/IbC61dx0q0vsPnzb+jwaHbubWFwdib3l50SUzEH2fMzHggr6FrrG7TWR2itC4GpwFqtdXBZuJWALybqos5rrM1pezmx8FVbIVa7w/j84GvPXutKdIjvHUZ9K8gpMLw+XOhjWVEZHu0JeY0djN43Z9mbTLz3Zf/ned8fyfY7JsdcxAORuuaxJ+LEIqXUrUCN1nol8GdgmVJqG7Abr/ALFohnqyaZK+CZ9S3SapF2N2P2YZaApLXmruc/5MhBfbn0tHzOGnkoz23+kj9PL2b8qDzb7+kJZM/P2GNr6Vtrvc7nP9da39gp5mitD2itf6i1PkZrfZrW+mM3GpuMiFUTP0QzKzFz4/z1wr+aWv4KxZziOf735WXm8YeJi3n2tWMYfsNqHlj3ETes2ITHo/nR6fnUzi+NWzEH2fMzHpDU/xgjVk18EemsJFwcfPDv2CfmD5Q+AEB7h4dz7nyeWx/zAN5YguKCQ3hoxrcTqr5KMs/qEgER9Bgju7XEluBNqHOzclk4aWHEom5n+7WyojLe/ayRQf0yOCynDx81ev3wM8cM539LRyWUkAvxgQh6HBAPVo1TtcQTiapNVcx4ckaXTM+G5gZmPjUTcDYjNvh3vOTljym8fhUAk04YyoPTTuWPE/pyzoSxjr1T6H2IoAsxSfOPFYEDV4pKMdwYorWjNeq9OI3o8Gh+/o+NrHjrsy7Hr5kwEoDMNLHIheiQaotCTEMnezJVPDjm30jMfTgZZbS/1Vsoa/uufX4xP/rQfrxRMYHa+aUcO7S/Y+8SejdioQs9Ejpp5NIBTGcGwxjm+LvthBU6EWX09id7+MH96wH4x5wz+XbhIB6e8W3GHD2YjDSxpQTnEUEXot5cIhxmLp2stCzTmcEjJz3iyrutkJGaEVWU0T9qPuHnj7/T5Vj/Pt5/ar6NlwXBDcRMSAKidVu4neZv5tLxRZYE4+TMwEplxcAa7LlZuTx0/kMR+8//8mptFzF/dNbp1M4v5VtDjapOC4KziIWe4DixoOl26KRdgXYyqSrcuxWKZRcus9XXLu6j/sdyZMd8du/N5J2bz2Hi8UN5actX3HTu8Rw5yLw+jCC4gQh6guPU5shuhk6auXRys3Jpbm82TqqyWNotXLhluJR8jbYt5rOfnk1bSy5HtjwJzal8AkAHO/ce4LCcLJZM/7bl5wmCk4jLJcHp6Vowkbh3zFw6CyctjP5kr9AAABy2SURBVCpV3KxS5VWrrvK3sam1yXDPTx9maflmVKypoF/TtRze8iAKb4XFPWnLIe9qDsuJn0JZQu9ELPQEx+0FzUAide+Ec+lEOjMwm50sqlnkL3jV0NxAeko62RnZNLU2dbnW6jqB1poNH+9mV1MLdY11ZKdupK/nTHZmVNKc6t0h6JtvJIZciD1ioSc4kS5oRmJpRxOvbnV7ODuYzUKC64y3edrIzcr1F8qyOhto6/Dw1Nufce59r3Dpnzbwh7VbyR+Qz960Z9iRNcUv5iDF1IT4QCz0BCeSBc1ILe14K/Vrp2RtXWOdrXWCf27+kptWbuaLxgMcfWg/7riwiAtOHsb/fSDF1IT4RSz0BMJnVatbFGm3pqFuURTeWwjQzfoNZYFHamnHW6lfo9lJYAhiIIOyBoV93ie791P/zQEAcrLSKcjty0OXFfPCtWdz6Wn59ElPlRKxQlwjgp4AVNdXk/2bbKatmOa3SH1p60Zb1oXb1i5SS9uue8doUHEy1d9IXOcUzyE9Jb3btXtb95q+6+1P9nD1o//h7Lte5P4XtwFw+lG5LJ99JuO+ldet6qEb7iNBcAJxucQ5VZuq+O2Hv6Vdt5teExymGC6UMdRCanAY4OQRk1m9dbX/8/TR07t8njxiMhVrKihfUd4lpT+wJC14B54ZT85AKUVrR6v/WLRFwIzcKI9tfqxb0pJRwa3q9+pZ9K+PqNnxNf37pPHjs47isu8URtQOQYgHxEIPg5vFo6w8u2JNRUgx9xFoXYezwM0s7ckjJnez7B+sebDL56Ubl1I5vhLPTR4qx1eydOPSLudnPDmDmU/NNMwCbfO0+cXchxtFwHY37zY8XtdYx4G2gwW5Xnivni+/OcCNU47j1RvGc8OkURJ6KCQ0IughCOe66IlnW11wDPRjm/mLfdeY+YFXb10dNk0+UICNZgJGoh2OUH2MZEA18umn6kMYqn/MGXes4a26rwH4Veko1s0rYeZ3h5OdKZNVIfERQQ+Bm2VlrT7byoJjoB+7alMVe1v3drsmPSW9i6/byA9sJ2Ik8P/RYtbHqk1VzHxqZpdBb+ZTM8OKeuX4Sr8fPd1TSG7rXIYdeIj0A1PIzPqEvhle8c7JSictVf4JCMmD/DWHwM0wPavPrhxfSZoytx6Doywq1lQYWsgDMgeEDWU0ixAJxifAdqNb0lPSu2VthlpUnfvs3G59ae1oZe6zc8O+q0N3oHQGh7fcR9+O/8fe1Of4PPMKXt8/i5qvVtpqtyAkCiLoIXAzTM/qs8uKyvjlsb8kNyvXf8yXJKNv0t2iLMwGCjO/so+KNRXdEnKMCBRgI1+8kWj72vzwDx7mofMfshzyZ1aN0ez43gNtlPzuKSqqBqI9aWjVSkP6fXzW5zK+zvgj7SlfAFgaEAQhERHHYQgqx7uXRGLn2RPyJnD7Jbdbem6kpQBCzToKcgpCpuwbbVwRKtHJ6TC/j79qYtLCl2lp9+D7k87wjKAldTNNac91u95sQBCEREcEPQRulpV169mRDkJmA0FBTgG119Sa3hdup/tIqa6vJkWl4NGebucCZytPb/ycn/ztLf/nxrTH2ZO2FFT42YYgJBsi6GFws6ys2bODY8GnHTaNEkosPxPsDxRuzkbsUrWpigVbFhiKeYpOIb15Cnm/uomsIUu5dWwlMJD7fnQyU048nMJ7r2ZPY2gxDxwQBCGZEEF3gXA1usPdG1xnZcHeBYzaNAqwJtSRDEJub3Jhh4o1FbR4Wroe1Okc2vpz+nq+4z+0Y8/tXLlqNovLFjOl6HDAeGAKJD0lnYWTFrrWdkGIJSLoDhPtDkJG4YwtnhbmPju3y2YQTmRZBhM8EPhiwM2yRt0S/GB//qDWOfTvmOL/3Kq2U59ZAcrTLUs2eGDyxeTvbt4d00FKEHoCEXSHiXYHIbPFSaOFvHDPdXqm8GDNg/7zbgwoPvJz8vlyd1+08tCa8iFtKZ9DBzSl/pOG9PtAdXXFBH9nbrrJBCGeEUF3mGhj1+2UhA31XDdmCsHsb9vP9CemW36mFR59rQ6+vJ+hnZ93ZE1hb9pKOrKqyUrLgubufnWpRS4IXiQOPYhoa7dEG7tuFNudmZJpupBn9txos1ytDkAdusORcgi3PL2ZwutX8asnNh08OOjuLvHqCyctjGgzD0HoLYQVdKVUH6XU60qpjUqpzUqpWwyuuUwp9ZVS6u3O/2a509zoCCfWTtRuiXQHIR9GdVYm5k00vDbUc52YKVgl0nII3xxoQ2tvRMrD62sBGDqgD/++fhyPTOxH7S/WdClNILXIBSE0VlwuLcA4rXWTUiodeEUp9azWekPQdX/XWv+P8010BisuiGj938HPinTxMNAHXLWpisufvLxb1EduVi4LJy00fW60e42GixYJxupAUbWpiorn7ocG7wDw/dHNLL70ItZfP47cfhn0SfduvLzF5P5wC7ey6Cn0ZsJa6NqLb3fd9M7/Ei5rw4oLIpxVa9UdE24DBDtuHcMQPmDPgT2Uryg3vd+NmcKVxVeSqlINr7cyUNyw6jEqqgb6xRzg8Q+XULWpimEDs/xiDt7EonDfkZvVMAUhEVG+KW/Ii5RKBd4EjgHu11r/Muj8ZcAdwFd4jatrtdafGDxnNjAbIC8v79Tly5fbbnBTUxPZ2dm27xv3r3GmtUoUiiGZQ2hub+abjm+6nc/LzGPW8Fks2LKgi7hmpmQyb+Q8JuRNsNyO6vpqW88J1e5w91fXV7Nk+xJ2tuykf2p/ULC3fS9DMocwa/issO0OvN93DxDR99DSrrmi+uCAujPjZppTawDv97v8jIN/C9X11Sz4cAEtOvQ7pm6YSn1Lfbd3BT8vUYj0bzuR6Y19huj6PXbs2De11sVG5ywJuv9ipQYCTwA/0Vq/G3A8F2jSWrcopa4ALtFajwv1rOLiYl1TU2P53T7WrVtHSUmJ7fsK7y0MGz2SkZqB1po2T5v/WN/0viw+dzEVayoiSo232g6z51hpd7h2BLub4GC/QoU8mt0D4V1K+1rauWLZm7yybRdbbp9ERloKA//3R+xLfZn2lE+7XKtQeG46GL1i9TtKuSXFcLALfl6iEOnfdiLTG/sM0fVbKWUq6LaiXLTWe4AXgYlBxxu09ptTS4BTI2momxi5IIJp7WhlQOYAw0U3p0rp2nlO1aYqdu3fFdVzIbKIl3DrCWYupbqG/Zx48/Mcf9PzvLLN2/Z3Pt0DwMDB/+4m5tDdXWP1O4q3TasFIdZYiXI5tNMyRymVBXwP+CDomsMCPp4HvO9kI50g2Cdsxu7m3YZi5ZR4WH2Ob3OHfW37bD3XyD8fydpAJAPYE299yll3vcg3B7xb5s0cM5yPfzOZ4kJvtqZVv77V7yjadQJBSDasWOiHAS8qpd4B3gBe0Fo/o5S6VSl1Xuc1P+0MadwI/BS4zJ3mRkegZVmQU2B4jZmYOCUeVp9jtlGFEb77jRYJy1eUm85MfJtCGy0shtvGzsefXvqYq6v+A8DkIu+4fvfFo6mdX8qN5x5HSsrBwdNq2GHl+EoyUzIN+xiIhDEKQlfChi1qrd8BTjY4fmPAzzcANzjbNOcwSoG3W13QrEZI+YpyKtZUhAyXC37/9NHTw9ZEserKUSi/iBXeW9jNTaLR7GvbR3pKere1gcrxlaaulay0LPqm9zX8flrbPVz32Ns8884X/nO/92gy01KpnV8asr1W0/IzUzL9C6+hQjQlzV8QDpL0maJmFijgt+4AUlWq30ccLhxx2YXLaG5vpqG5IWy4nNH7l25cSuX4StOwRrDuyklPTff/HGoQsLs2sLt5t6H1+8FHoxn5v8/6xfzoQ/tR878TSE2xtn1dOHzf1zftB6ONmtubHXm2ICQ7SS/o4Rb3fC6QDt0BYCmW2c4iY6Qp+JXjKw23cgumtaPV/6xQg0AkawO+Aeytmbt5+qKNlBWVMTSnDwAXnjyMLbdPYs3PShicnWn4jEhwc2NuQUh2kl7QzSzQHY07/K4QuwJiZ8Ew0uiYsqIyHjr/IUubMfieFcqfb+YTD+XT/0fNJxRev4rz71/Pufe9AsBVJUdTO7+Uuy85iYw05/983NyYWxCSnaQX9FBWq88VYkQoATF7ZopK6WbZRxMdU1ZUxq5f7OLFs19E36RNF3J97y0rKqNfer+wzw1+R7Br5YJhf6GiaiA/f/wd/3WP/vh0AJRyxrVihoQiCkLkJL2gh4o/39+2P6JUdrNnGlUedDK0zsp7zWqv7G7ebfrcsqIyNl+5jY4bO6i9ppZX3ve6VQb1y+DlX4yldn4p3zl6sO32RoKEIgpC5CS9oPssUDM6dIdtAfE902gwCHbXOBlaZ+W9di3crfV7OeqGVRx/0/M88u9aAF76+Vjev3Ui//n19zhyUOhkrGgwin/39TEvM09CEQXBJr1ig4uyorKQqfu+8D07FfvKisooX1FueG5H445uFQDtlAeI5r25WbmmIYqB/HPzl8xe9maXYzlZ3oiZ/Fz3RNxHuOqXwxqG9cqUcEGIhqS30H2EmsqHq45ohpnVq1CuVgAM5Q5qaG5AKUVuVq6phbv3QFsXMV807VRq55dy4SlHONbGcEg0iyA4T68RdDeyCo0GCYXqVjDKaaEKV5emtaOV7Ixs/wB14bFTOef3T1J4/SpSbk6jaNEIzj7+AP+89ixq55cy8YShps9yC4lmEQTn6TWCDl1T/31ulki3mvM9L3iQMCt166RQBb7XjLrGOj7b00zx7dWMuvE5Pvzc605J10ezo3EHj3/y37yxc6VjbbJLNNEs0W4TKAjJSq8SdB9G2Zszn5rJ4DsH2xYJX3JSfk4+dY11plEzGu2o+PgGJ9OaNBk/ZMz8texq8qbP7019hh19zqM1xbsXUKzdG5FGs8imFoJgTsILeiTWmpH/trWj1VIqv9H7AwXGl3FqhBviEyiM/dunkNt6DX3T+3LzRG/dtDv/60Tqss5ld8YiUF1rhMfSvRGpC0x874JgTkJHuVjZJ9QIK0JmdS9RI4EBb20YI3G3u0dpOC4+7lIe/3df3tp+sEzAPVOKKT+xjMs6S6r9Yn10+4u6RSSFtcT3LgjmJLSFHqm1ZlXIrIiE2TUe7TGtu76jcYcj/t8bn3qXERXP+sU8f1BfXv/VeMpP7F6ONlmSdSSTVBDMSWhBj9Ras7J7EVgTiVACE+r+SP2/737WyAdfeisR9u/jnWCVnngYH94+kZd+MZYhA/p0uyeZ6oYn0+AkCE6T0C6X/JzIXAllRWWsr1vPoppFplEpVkUiXF314HPBWHXBPPX2Z8xd/rb/c+38UuZ9/1jmff9YS/VVkqVueHBdequJYILQG0hoQbezSUXwJhNNrU2mYu7LHgW6ZXwGC0c4gQk3cEDoGcVvn/uAB9ftAw6K+dKZpwHuF8qKV5JlcBIEp0loQbdqrRktnpqhUNReU2t4T/mKctbXreeB0ge6tcNMYFZvXR1SzKH7jKK5tYM+6SkopXhw3UcA9M9M45mffpeCXHvVFAVB6D0ktKCDNWvNLBLFCJ+4Gt2j0SyqWcSY/DGWLcRw/vzAGcXHXzUxaeHLtLR7mH9hEVNPy2fNz85m68Y3mDhhrKX3CYLQe0noRVGrWA1pCxRXs3s02jCKxiwePpQ/37c4eXj69yi8fhXjfvcvWtq9seLpqd5fzdGHZtMnrXe6VgRBsEevEPRQopqqUg0jP0Ld49vtyEeo7EWzqIy/XvhXaq+pZcoxFzPjkTf85+770cnUzi/lv041L5Qlqe+CIBiR8C4XK1SOr2TaimmG5zzag+cmT7fjleMrKV9Rbur/nv30bNbXrWf11tWGPnlf9IqvbK7fzz/gKEal30VFVQZTf6PJyUpn5pjhXHTqERx3+ICwfYk0mUoQhOSnV1joZUVlpntzmlniZUVlzCmeY5octL9tP4tqFoVcYPW5bcqKynh1xvucnvEC1C/k/U+9iUCbP28E4MZzj7Mk5iCp74IgmNMrBB1g4aSFthNSHih9gGUXLjM9bzV6ZfnrdZx5x1q+aDwAwMXFR7CtchInHjHQavP9SOq7IAhmJKyg2/UjR5otWVZUFrJMrRmD9Lmc1v9uAM4dfTgAt/3gBGrnl3LnRaNJS43sq5fUd0EQzEhIH3qkfmSrCSnBSUiTR0xm6calXVwdRhtZoFMY1HY1/TvOAeD1reDxaPplplE7v9RuNw2xk0wlCELvIiEtdDf9yEYRK0s3LmX66OldrPs5xXO6uHAGtc6h4MBKv5i300BD9pX8bfOjUbcpkGSqyyIIgrMkpIXuph/ZbLBYvXV1t42ej+7///jD6/dSt+8Nf63x/SmvsivjLrRqhQ5slcoNnhmY1SiR1HdBEIxISEGPtCiXFawMFqs3fcFVVf8BBgA34plfSsrNKexOW0xwUIzVQSaUG2kYw2z3QxCE3kdYl4tSqo9S6nWl1Eal1Gal1C0G12Qqpf6ulNqmlHpNKVXoRmN9RFtCNdSCaqhFx7v/+SGF16/qFHMvD11W7D0/ML+bmId6XjASjigIQrRY8aG3AOO01qOBk4CJSqkzgq65HPhaa30McA/wW2eb2ZVo/Mjh9qTsNljoNP9g8fu12wDITEthzc/OpnZ+KeO+lWd8H/YGmZ4KR5QsU0FIXsK6XLTWGmjq/Jje+V9wAPb5wM2dPz8O3KeUUp33uoIVP7KRTzqUJRz4zIrn78az6xekkM3Fp+yjrOhiiq/Zy2ED+zCgT7pheyDyOt1uupF8SJapICQ3yormKqVSgTeBY4D7tda/DDr/LjBRa/1p5+ePgNO11ruCrpsNzAbIy8s7dfny5bYb3NTURHZ2dtjrquurWbBlAS2eFv+xzJTMLp+7tA3F2rPX8u6uDhbUHOhyblZRBt8d1l3EncSsvfNGzuOMfmdY6nPgs5ZsX8LOlp0MyRzCrOGzmJA3gakbplLfUt/t+rzMPJafYf934SZWf8/JhPS59xBNv8eOHfum1rrY6JylRVGtdQdwklJqIPCEUuoErfW7dhuitV4MLAYoLi7WJSUldh/BunXrMLsv0CJPUSndNmlu8bSYbt6cn5PP6G9/h8tue8F/7J5LRnPByeZFspykhBJGbRplaOGH6nMwVZuquOff9/it8PqWeu756B5GHTeKnS07De/Z2bLT8vN7Cjt9Thakz70Ht/ptK8pFa71HKfUiMBEIFPTPgCOBT5VSaUAO0OBYKy0Q7E4wEm3f8fSUdNo8baDTGNx2Hf06zuK2C/ZwSL8MfnR6Ppd+O5+iI3IcbVtPhSOGcin1hFtHEITYYSXK5dBOyxylVBbwPeCDoMtWAtM7f74IWOum/9wIO5tYpOpDOLLlzxQceJJ+HWcBcOrgcwH4zQVFjot58CLsjCdnMPjOwa4sTIZaXJUNlgUhubES5XIY8KJS6h3gDeAFrfUzSqlblVLndV7zZyBXKbUNuA643o3GVm2qYuqGqYZCaDUaJLt9Inn7HyLF441OufDkYWytnGS52qFdjAaaNk8bDc0NhlE20RIq7FKyTAUhuQkr6Frrd7TWJ2utT9Ran6C1vrXz+I1a65WdPx/QWv9Qa32M1vo0rfXHTjfUZ+nWt9QbCmEot0G/9vEc0nY5APtSXwZgd/piaueXcvclJ/l3Bwp+nxPhfVYGGifjzUNZ4VZdP4IgJCYJU8slXOJN9/jxFIa0/4SC5mcY3HYtA9ovAA1a7WNH1hQG5W40fVe4WHU7WPVPOxVvbmaFA471SRCE+CRhUv/DJd4ExoHv/Woi/TvO9V/jUY18nnGtP5MznN84XKy6HYyqIxrh5MKk0eJq4b2FjvVJEIT4JGEs9HB1wLft3Mt3D/8BtdfU8uNTrgKg5NhDef/WidzxI80Rh2RZ9hs7mbUZbDHnZuWSkZrR5ZqeWJiUjTEEIflJGAvdrA542cg7Kbx+lf9Y7fxSfnNBEXdcWIRSXpM8XDhgsG95UNYgGpq7R11GakUHvz8WvmwJWRSE5CdhBN0neD9b9TN2tuwkP/UK+GYKVS8dvOaP5acCkJJivA+oEUbp8BmpGQdj1Ttx0oqORflb2RhDEJKfhBF08ArhsIZhlJSU+K1ypeD5a85iZF7/iJ5p5C9v7WglNyuX7IzspIkIibbWjCAI8U9CCXogz/zkuxx5SF9y+kZXY8XMh7y7eTe7frHL8FyiIhtjCEJykzCLosGcMCwnajEH2XRZEITkIeEEvbq+2tF63pIOLwhCspBQgl61qYoFWxZ0SY4pX1HOVauuiviZkg4vCEKykFA+9Io1Fd3qmWs0i2oWMSZ/TMQiLL5lQRCSgYSy0M0WMDVa9t4UBKHXk1CCHmqhUjIeBUHo7SSUoIdaqJSoFEEQejsJJehlRWWcf9j5KLpmgkpUiiAIQoIJOsA1I69h2YXLJCpFEAQhiISKcvEhUSmCIAjdSTgLXRAEQTBGBF0QBCFJEEEXBEFIEkTQBUEQkgQRdEEQhCRBaa1j82KlvgK674kWnsFAchUqD4/0uXcgfe49RNPvAq31oUYnYibokaKUqtFaF8e6HT2J9Ll3IH3uPbjVb3G5CIIgJAki6IIgCElCIgr64lg3IAZIn3sH0ufegyv9TjgfuiAIgmBMIlrogiAIggEi6IIgCElCXAq6UmqiUupDpdQ2pdT1BuczlVJ/7zz/mlKqsOdb6SwW+nydUuo9pdQ7Sqk1SqmCWLTTacL1O+C6/1JKaaVUwoe4WemzUurizt/3ZqXUoz3dRqex8Pedr5R6USn1Vuff+ORYtNNJlFIPKaV2KqXeNTmvlFK/7/xO3lFKnRL1S7XWcfUfkAp8BBwFZAAbgeOCrrkKWNT581Tg77Fudw/0eSzQt/PnKxO9z1b73Xldf+AlYANQHOt298DvegTwFnBI5+chsW53D/R5MXBl58/HAbWxbrcD/T4LOAV41+T8ZOBZQAFnAK9F+854tNBPA7ZprT/WWrcCy4Hzg645H1ja+fPjwHillCJxCdtnrfWLWuv9nR83AEf0cBvdwMrvGuA24LfAgZ5snEtY6fOPgfu11l8DaK139nAbncZKnzUwoPPnHODzHmyfK2itXwJ2h7jkfOAv2ssGYKBS6rBo3hmPgj4M+CTg86edxwyv0Vq3A41Abo+0zh2s9DmQy/GO7IlO2H53TkOP1Fqv6smGuYiV3/VIYKRSar1SaoNSamKPtc4drPT5ZmCaUupTYDXwk55pWkyx++8+LAm5Y1FvRik1DSgGzo51W9xGKZUC3A1cFuOm9DRpeN0uJXhnYi8ppYq01nti2ip3uRR4RGv9O6XUmcAypdQJWmtPrBuWSMSjhf4ZcGTA5yM6jxleo5RKwztFa+iR1rmDlT6jlJoAVADnaa1beqhtbhKu3/2BE4B1SqlavH7GlQm+MGrld/0psFJr3aa13g5swSvwiYqVPl8OPAagtX4V6IO3gFUyY+nfvR3iUdDfAEYopYYrpTLwLnquDLpmJTC98+eLgLW6c5UhQQnbZ6XUycAf8Yp5ovtUfYTst9a6UWs9WGtdqLUuxLt2cJ7WuiY2zXUEK3/fT+K1zlFKDcbrgvm4JxvpMFb6XAeMB1BKjcIr6F/1aCt7npXAf3dGu5wBNGqtv4jqibFeCQ6x+rsF78p4ReexW/H+YwbvL/sfwDbgdeCoWLe5B/pcDdQDb3f+tzLWbe6Jfgddu44Ej3Kx+LtWeF1N7wGbgKmxbnMP9Pk4YD3eCJi3ge/Hus0O9PlvwBdAG95Z1+XAHGBOwO/5/s7vZJMTf9uS+i8IgpAkxKPLRRAEQYgAEXRBEIQkQQRdEAQhSRBBFwRBSBJE0AVBEJIEEXRBEIQkQQRdEAQhSfj/aNTAFYt8ijYAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight :  tensor([[1.7906]])\n",
            "linear.bias :  tensor([2.8342])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 Stochastic Gradient Descent\n",
        "We can run the same optimization, but on chunks of the data, a.k.a. minibatches. This variant of gradient descent is then called Stochastic Gradient Descent due to the stochastic nature of optimizing $\\vartheta$ on subsets of the data."
      ],
      "metadata": {
        "id": "CzNNZ8iP7irg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_batches = len(x) // batch_size\n",
        "batch_idxs = np.arange(num_batches)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    np.random.shuffle(batch_idxs)\n",
        "\n",
        "    for i in batch_idxs:\n",
        "        # slice out the portion of x and y, which corresponds to the batch i\n",
        "        x_batch = x_train[batch_size*i:batch_size*(i+1)]\n",
        "        y_batch = y_train[batch_size*i:batch_size*(i+1)]\n",
        "        \n",
        "        inputs = Variable(torch.from_numpy(x_batch))\n",
        "        y_train_var = Variable(torch.from_numpy(y_batch))\n",
        "\n",
        "        # Clear gradient buffer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Output from model given the inputs\n",
        "        y_pred = model(inputs)\n",
        "\n",
        "        # Get loss for the model's prediction\n",
        "        loss = criterion(y_pred, y_train_var)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-j7GjPQ66RC",
        "outputId": "8dfb6f28-c28e-4d13-97d2-a3d5dcd734f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 0.1341165155172348\n",
            "epoch 1, loss 0.11462900042533875\n",
            "epoch 2, loss 0.06527148187160492\n",
            "epoch 3, loss 0.09232383966445923\n",
            "epoch 4, loss 0.06726282835006714\n",
            "epoch 5, loss 0.051201459020376205\n",
            "epoch 6, loss 0.06898342818021774\n",
            "epoch 7, loss 0.05608513951301575\n",
            "epoch 8, loss 0.05525144189596176\n",
            "epoch 9, loss 0.048698026686906815\n",
            "epoch 10, loss 0.05908149108290672\n",
            "epoch 11, loss 0.04924646019935608\n",
            "epoch 12, loss 0.05677725374698639\n",
            "epoch 13, loss 0.04949120804667473\n",
            "epoch 14, loss 0.07184114307165146\n",
            "epoch 15, loss 0.09527931362390518\n",
            "epoch 16, loss 0.05539153143763542\n",
            "epoch 17, loss 0.07136432826519012\n",
            "epoch 18, loss 0.0942763090133667\n",
            "epoch 19, loss 0.04977710172533989\n",
            "epoch 20, loss 0.053703874349594116\n",
            "epoch 21, loss 0.05027631297707558\n",
            "epoch 22, loss 0.062108077108860016\n",
            "epoch 23, loss 0.09364777058362961\n",
            "epoch 24, loss 0.05040581151843071\n",
            "epoch 25, loss 0.05419829115271568\n",
            "epoch 26, loss 0.04994799196720123\n",
            "epoch 27, loss 0.05491425096988678\n",
            "epoch 28, loss 0.054224707186222076\n",
            "epoch 29, loss 0.06246836110949516\n",
            "epoch 30, loss 0.05334421992301941\n",
            "epoch 31, loss 0.05065721273422241\n",
            "epoch 32, loss 0.050673991441726685\n",
            "epoch 33, loss 0.05472061038017273\n",
            "epoch 34, loss 0.05074175447225571\n",
            "epoch 35, loss 0.050022538751363754\n",
            "epoch 36, loss 0.05463053286075592\n",
            "epoch 37, loss 0.0533229261636734\n",
            "epoch 38, loss 0.05422830954194069\n",
            "epoch 39, loss 0.050792451947927475\n",
            "epoch 40, loss 0.05344318225979805\n",
            "epoch 41, loss 0.054487381130456924\n",
            "epoch 42, loss 0.054230041801929474\n",
            "epoch 43, loss 0.053417522460222244\n",
            "epoch 44, loss 0.04994110018014908\n",
            "epoch 45, loss 0.05438035726547241\n",
            "epoch 46, loss 0.05421440303325653\n",
            "epoch 47, loss 0.09285695105791092\n",
            "epoch 48, loss 0.053617894649505615\n",
            "epoch 49, loss 0.0707751139998436\n",
            "epoch 50, loss 0.07075498253107071\n",
            "epoch 51, loss 0.06236936151981354\n",
            "epoch 52, loss 0.09274177253246307\n",
            "epoch 53, loss 0.05103391781449318\n",
            "epoch 54, loss 0.049883175641298294\n",
            "epoch 55, loss 0.07071450352668762\n",
            "epoch 56, loss 0.05107172951102257\n",
            "epoch 57, loss 0.062433041632175446\n",
            "epoch 58, loss 0.049862444400787354\n",
            "epoch 59, loss 0.054040759801864624\n",
            "epoch 60, loss 0.054213352501392365\n",
            "epoch 61, loss 0.05369678512215614\n",
            "epoch 62, loss 0.0511748380959034\n",
            "epoch 63, loss 0.062413983047008514\n",
            "epoch 64, loss 0.049845319241285324\n",
            "epoch 65, loss 0.05118491128087044\n",
            "epoch 66, loss 0.05376260727643967\n",
            "epoch 67, loss 0.07066934555768967\n",
            "epoch 68, loss 0.049842141568660736\n",
            "epoch 69, loss 0.0706787109375\n",
            "epoch 70, loss 0.062341365963220596\n",
            "epoch 71, loss 0.049801409244537354\n",
            "epoch 72, loss 0.0538986474275589\n",
            "epoch 73, loss 0.06228562071919441\n",
            "epoch 74, loss 0.051274001598358154\n",
            "epoch 75, loss 0.06224950775504112\n",
            "epoch 76, loss 0.04977015033364296\n",
            "epoch 77, loss 0.054209526628255844\n",
            "epoch 78, loss 0.053664155304431915\n",
            "epoch 79, loss 0.053646791726350784\n",
            "epoch 80, loss 0.05420641601085663\n",
            "epoch 81, loss 0.0536116287112236\n",
            "epoch 82, loss 0.06228922680020332\n",
            "epoch 83, loss 0.053578171879053116\n",
            "epoch 84, loss 0.05356137081980705\n",
            "epoch 85, loss 0.07067152112722397\n",
            "epoch 86, loss 0.054209865629673004\n",
            "epoch 87, loss 0.05422786623239517\n",
            "epoch 88, loss 0.05349719524383545\n",
            "epoch 89, loss 0.05149243399500847\n",
            "epoch 90, loss 0.06222735345363617\n",
            "epoch 91, loss 0.054081618785858154\n",
            "epoch 92, loss 0.05423048138618469\n",
            "epoch 93, loss 0.0920984148979187\n",
            "epoch 94, loss 0.053405970335006714\n",
            "epoch 95, loss 0.054128170013427734\n",
            "epoch 96, loss 0.062202148139476776\n",
            "epoch 97, loss 0.062181506305933\n",
            "epoch 98, loss 0.04973100125789642\n",
            "epoch 99, loss 0.062198761850595474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the performance of the model **after** optimization:"
      ],
      "metadata": {
        "id": "VgbKaZ8X53FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data')\n",
        "plt.plot(x_train, predicted, '--', label='Predictions')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  print(name, \": \", param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NTq3wWFn6Dgn",
        "outputId": "b7439103-ac18-47d8-f0dd-619559a4e3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight :  tensor([[1.9625]])\n",
            "linear.bias :  tensor([3.0317])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.4 Analytical Solution"
      ],
      "metadata": {
        "id": "0_YtuxIRzGHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw in the lecture, the linear regression problem is one of the very few machine learning algorithms which admits an analytical solution. This reads\n",
        "\n",
        "$$\n",
        "\\quad\\vartheta=\\left(X^{\\top}X\\right)^{-1}X^{\\top}Y\n",
        "$$\n",
        "\n",
        "> Caution: To get the so called bias term $\\vartheta_0$, we need to extend $X$ to \n",
        "\n",
        "$$X_{m \\times n}=\\left[\\begin{array}{c}x^{(1) \\top }\\\\ \\vdots \\\\ x^{(m) \\top} \\\\ \\mathbf{1}^{\\top}\\end{array}\\right],$$\n",
        "\n",
        "otherwise we assume that the line we are fitting crosses $y$ at $x=0$. The PyTorch model we saw before automatically defines a bias term and optimizes it to the data."
      ],
      "metadata": {
        "id": "e4VmdXOTzL8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extended x vector\n",
        "x_ext = np.ones((x.shape[0], 2))\n",
        "x_ext[:,0] = x"
      ],
      "metadata": {
        "id": "TYoBlkN5GUGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtx = x_ext.T.dot(x_ext)\n",
        "xtx_inv = np.linalg.inv(xtx)\n",
        "xtx_inv_xt = xtx_inv.dot(x_ext.T)\n",
        "theta = xtx_inv_xt.dot(y)\n",
        "\n",
        "print(\"theta =\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-4UOpRZzJPR",
        "outputId": "4c419a41-db71-4bbe-ad03-52210cdd334d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta = [1.98897946 3.01789534]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.5 Exercise"
      ],
      "metadata": {
        "id": "jwOhkK3nBdUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply linear regression on the housing price dataset provided [here](https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction) using:\n",
        "1. Gradien Descent\n",
        "2. Stochastic Gradient Descent\n",
        "3. Analytical Solution\n",
        "\n",
        "> Note: there won't be a solution to this exercise. It is only provided as practice material.\n",
        "\n",
        "> Hint: You might find [this](https://www.kaggle.com/code/aminizahra/linear-regression) helpful.\n"
      ],
      "metadata": {
        "id": "waZo7E_TBnA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# TODO\n",
        "\n",
        "\n",
        "####################"
      ],
      "metadata": {
        "id": "aTWktpkf0J3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Logistic Regression"
      ],
      "metadata": {
        "id": "mK7nDoX3J7P5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem setting\n",
        "- Given: given is a set of measurement pairs $\\left\\{x^{(i)}, y^{\\text {(i)}}\\right\\}_{i=1,...m}$ with $x \\in \\mathbb{R}^{n}$ and $y \\in \\{0,1\\}$\n",
        "- Question:  if a give you a novel $x$, what would be your best guess about its corresponding $y$? Up until here, the only difference to linear regression is in the domain of $y$.\n",
        "- Logistic regression assumption: Instead of asking directly whether the class is 0 or 1, we model the ***probability of the class being 1*** with $h$:\n",
        " $$h(x) = \\varphi \\left( \\vartheta^{\\top} x \\right) = \\frac{1}{1+e^{-\\vartheta^{\\top} x}} = \\frac{1}{1+e^{-(\\vartheta_0 + \\vartheta_1 \\cdot x_1 ... + \\vartheta_n \\cdot x_n)}} $$\n",
        "\n",
        "Sigmoid function:\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1280px-Logistic-curve.svg.png\" alt=\"drawing\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        " > Note: Unfortunately, even this very simple classification model does not have an analytical solution, thus we use gradient-based optimization.\n",
        "\n",
        " Reference: this implementation is a simplification of the example given [here](https://blog.jovian.ai/torch-logistic-regression-on-iris-dataset-d966b23339da)."
      ],
      "metadata": {
        "id": "Mj2gB2PQKXcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Iris Dataset"
      ],
      "metadata": {
        "id": "J_LE9NtFS4au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get iris dataset\n",
        "from urllib.request import urlretrieve\n",
        "iris = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "urlretrieve(iris)\n",
        "df0 = pd.read_csv(iris, sep=',')\n",
        "\n",
        "# name columns\n",
        "attributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
        "df0.columns = attributes\n",
        "\n",
        "# add species index\n",
        "species = list(df0[\"class\"].unique())\n",
        "df0[\"class_idx\"] = df0[\"class\"].apply(species.index)\n",
        "\n",
        "print(df0.head())\n",
        "print(\"Count occurence of each class:\")\n",
        "print(df0[\"class\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3VDnFjZ2Q_q",
        "outputId": "36587171-ea86-44c1-a1dd-38d628757692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
            "0           4.9          3.0           1.4          0.2  Iris-setosa   \n",
            "1           4.7          3.2           1.3          0.2  Iris-setosa   \n",
            "2           4.6          3.1           1.5          0.2  Iris-setosa   \n",
            "3           5.0          3.6           1.4          0.2  Iris-setosa   \n",
            "4           5.4          3.9           1.7          0.4  Iris-setosa   \n",
            "\n",
            "   class_idx  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          0  \n",
            "4          0  \n",
            "Count occurence of each class:\n",
            "Iris-versicolor    50\n",
            "Iris-virginica     50\n",
            "Iris-setosa        49\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your can learn more about this well established dataset [here](https://archive.ics.uci.edu/ml/datasets/Iris). In essence, we see measurements of 4 different features and the corresponding type of iris plant out of ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']. We transform this problem to a logistic regression problem by looking only at two of the classes, which we denote with [0,1]. In addition, we consider only two of the features to make visualization possible."
      ],
      "metadata": {
        "id": "MKwD-D3ATN9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df0[[\"petal_length\", \"petal_width\", \"class_idx\"]]\n",
        "df = df[df[\"class_idx\"] != 0]\n",
        "df[\"class_idx\"] = df[\"class_idx\"] - 1\n",
        "\n",
        "print(df[\"class_idx\"].value_counts())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcczeCZVXVXg",
        "outputId": "4776cbf5-8ab3-4b76-a5b0-8a8bfcb82eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    50\n",
            "1    50\n",
            "Name: class_idx, dtype: int64\n",
            "     petal_length  petal_width  class_idx\n",
            "49            4.7          1.4          0\n",
            "50            4.5          1.5          0\n",
            "51            4.9          1.5          0\n",
            "52            4.0          1.3          0\n",
            "53            4.6          1.5          0\n",
            "..            ...          ...        ...\n",
            "144           5.2          2.3          1\n",
            "145           5.0          1.9          1\n",
            "146           5.2          2.0          1\n",
            "147           5.4          2.3          1\n",
            "148           5.1          1.8          1\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter_3d(df[[\"petal_length\",\"petal_width\",\"class_idx\"]],\n",
        "                    x = 'petal_length',\n",
        "                    y = 'petal_width',\n",
        "                    z = 'class_idx',\n",
        "                    color = 'class_idx',\n",
        "                    opacity = 0.9)\n",
        "\n",
        "fig.update_layout(margin = dict(l=0, r=0, b=0, t=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "nUP_b_wpV0AU",
        "outputId": "2d59f454-b6d7-45ef-8b87-ce187de50b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"ad71fddb-03dc-49f2-a330-691ce5e94465\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ad71fddb-03dc-49f2-a330-691ce5e94465\")) {                    Plotly.newPlot(                        \"ad71fddb-03dc-49f2-a330-691ce5e94465\",                        [{\"hovertemplate\":\"petal_length=%{x}<br>petal_width=%{y}<br>class_idx=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"coloraxis\":\"coloraxis\",\"opacity\":0.9,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[4.7,4.5,4.9,4.0,4.6,4.5,4.7,3.3,4.6,3.9,3.5,4.2,4.0,4.7,3.6,4.4,4.5,4.1,4.5,3.9,4.8,4.0,4.9,4.7,4.3,4.4,4.8,5.0,4.5,3.5,3.8,3.7,3.9,5.1,4.5,4.5,4.7,4.4,4.1,4.0,4.4,4.6,4.0,3.3,4.2,4.2,4.2,4.3,3.0,4.1,6.0,5.1,5.9,5.6,5.8,6.6,4.5,6.3,5.8,6.1,5.1,5.3,5.5,5.0,5.1,5.3,5.5,6.7,6.9,5.0,5.7,4.9,6.7,4.9,5.7,6.0,4.8,4.9,5.6,5.8,6.1,6.4,5.6,5.1,5.6,6.1,5.6,5.5,4.8,5.4,5.6,5.1,5.1,5.9,5.7,5.2,5.0,5.2,5.4,5.1],\"y\":[1.4,1.5,1.5,1.3,1.5,1.3,1.6,1.0,1.3,1.4,1.0,1.5,1.0,1.4,1.3,1.4,1.5,1.0,1.5,1.1,1.8,1.3,1.5,1.2,1.3,1.4,1.4,1.7,1.5,1.0,1.1,1.0,1.2,1.6,1.5,1.6,1.5,1.3,1.3,1.3,1.2,1.4,1.2,1.0,1.3,1.2,1.3,1.3,1.1,1.3,2.5,1.9,2.1,1.8,2.2,2.1,1.7,1.8,1.8,2.5,2.0,1.9,2.1,2.0,2.4,2.3,1.8,2.2,2.3,1.5,2.3,2.0,2.0,1.8,2.1,1.8,1.8,1.8,2.1,1.6,1.9,2.0,2.2,1.5,1.4,2.3,2.4,1.8,1.8,2.1,2.4,2.3,1.9,2.3,2.5,2.3,1.9,2.0,2.3,1.8],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"petal_length\"}},\"yaxis\":{\"title\":{\"text\":\"petal_width\"}},\"zaxis\":{\"title\":{\"text\":\"class_idx\"}}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"class_idx\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":0,\"l\":0,\"r\":0,\"b\":0}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ad71fddb-03dc-49f2-a330-691ce5e94465');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Preprocess and Dataloader"
      ],
      "metadata": {
        "id": "P0mK6L593I5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_columns = torch.from_numpy(df[['petal_length', 'petal_width']].to_numpy()).type(torch.float32)\n",
        "output_columns = torch.from_numpy(df['class_idx'].to_numpy()).type(torch.float32)\n",
        "output_columns = output_columns.reshape(-1, 1)\n",
        "\n",
        "print(\"Input columns: \", input_columns.shape, input_columns.dtype)\n",
        "print(\"Output columns: \", output_columns.shape, output_columns.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVqBPGu6cTtO",
        "outputId": "6ac6bfa2-c31c-47dc-95c6-3a4df562c1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input columns:  torch.Size([100, 2]) torch.float32\n",
            "Output columns:  torch.Size([100, 1]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set hyperparameters\n",
        "batch_size = 25\n",
        "\n",
        "# create a PyTorch data object used by DataLoader\n",
        "data = TensorDataset(input_columns, output_columns)\n",
        "\n",
        "# define data loader which shuffles the data\n",
        "train_loader = DataLoader(data, batch_size, shuffle = True)\n",
        "\n",
        "# one batch of training data would look like this:\n",
        "for x in train_loader:\n",
        "    print (x, x[0].shape, x[0].dtype)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6vVNjQKeKTe",
        "outputId": "01bdce2b-866e-459c-bad5-f37473f6d9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[5.8000, 2.2000],\n",
            "        [5.6000, 1.4000],\n",
            "        [4.4000, 1.4000],\n",
            "        [5.1000, 1.9000],\n",
            "        [5.7000, 2.5000],\n",
            "        [4.4000, 1.3000],\n",
            "        [5.8000, 1.6000],\n",
            "        [4.8000, 1.8000],\n",
            "        [5.1000, 2.0000],\n",
            "        [4.9000, 1.8000],\n",
            "        [5.0000, 2.0000],\n",
            "        [4.7000, 1.4000],\n",
            "        [6.7000, 2.0000],\n",
            "        [4.9000, 1.5000],\n",
            "        [5.5000, 2.1000],\n",
            "        [6.9000, 2.3000],\n",
            "        [4.7000, 1.6000],\n",
            "        [5.8000, 1.8000],\n",
            "        [4.9000, 1.5000],\n",
            "        [3.0000, 1.1000],\n",
            "        [6.0000, 1.8000],\n",
            "        [5.7000, 2.1000],\n",
            "        [5.6000, 2.4000],\n",
            "        [3.9000, 1.2000],\n",
            "        [3.3000, 1.0000]]), tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])] torch.Size([25, 2]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 Model and Training"
      ],
      "metadata": {
        "id": "KWxPLcdQ3w95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the core part of the logistic regression. Here we define the linear \n",
        "# transformation of x and afterwards pushing it through sigmoid\n",
        "\n",
        "# Define model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = torch.sigmoid(self.linear1(x))\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "pNkK9v_VPg6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [`torch.nn.BCELoss(h(x),y)`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) function implements $-\\log p(y|x;\\vartheta) = - \\log \\left(h^y(x)(1-h(x))^{1-y}\\right)$. Maximizing the probability is the same as minimizing this loss."
      ],
      "metadata": {
        "id": "-Jlfw-iIVVvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set hyperparameters\n",
        "learning_rate = 1.0\n",
        "epochs = 1000\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "# instantiating the model\n",
        "model = LogisticRegression(input_dim, output_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "qldErN24KXwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: {}. Loss: {}.\".format(epoch + 1, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W6h-6jReZ24",
        "outputId": "57a77e51-cbd8-4b8f-8328-8e8cb0ebc59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1. Loss: 4.938625812530518.\n",
            "Epoch: 2. Loss: 7.965576171875.\n",
            "Epoch: 3. Loss: 7.535805702209473.\n",
            "Epoch: 4. Loss: 0.5870006084442139.\n",
            "Epoch: 5. Loss: 4.353271961212158.\n",
            "Epoch: 6. Loss: 3.7897255420684814.\n",
            "Epoch: 7. Loss: 1.2760947942733765.\n",
            "Epoch: 8. Loss: 0.7826815247535706.\n",
            "Epoch: 9. Loss: 1.2233582735061646.\n",
            "Epoch: 10. Loss: 2.7424991130828857.\n",
            "Epoch: 11. Loss: 4.833599090576172.\n",
            "Epoch: 12. Loss: 0.5743933320045471.\n",
            "Epoch: 13. Loss: 4.14581298828125.\n",
            "Epoch: 14. Loss: 4.302636623382568.\n",
            "Epoch: 15. Loss: 2.0549697875976562.\n",
            "Epoch: 16. Loss: 0.8796350955963135.\n",
            "Epoch: 17. Loss: 1.1095879077911377.\n",
            "Epoch: 18. Loss: 1.9364012479782104.\n",
            "Epoch: 19. Loss: 5.856472015380859.\n",
            "Epoch: 20. Loss: 1.4038196802139282.\n",
            "Epoch: 21. Loss: 0.7667605876922607.\n",
            "Epoch: 22. Loss: 0.3182474374771118.\n",
            "Epoch: 23. Loss: 1.3136448860168457.\n",
            "Epoch: 24. Loss: 1.2768386602401733.\n",
            "Epoch: 25. Loss: 2.9474337100982666.\n",
            "Epoch: 26. Loss: 1.9164294004440308.\n",
            "Epoch: 27. Loss: 2.1767423152923584.\n",
            "Epoch: 28. Loss: 2.0053164958953857.\n",
            "Epoch: 29. Loss: 3.492051362991333.\n",
            "Epoch: 30. Loss: 3.4805638790130615.\n",
            "Epoch: 31. Loss: 2.524965286254883.\n",
            "Epoch: 32. Loss: 1.6103564500808716.\n",
            "Epoch: 33. Loss: 0.4974066913127899.\n",
            "Epoch: 34. Loss: 0.2495117336511612.\n",
            "Epoch: 35. Loss: 2.176064968109131.\n",
            "Epoch: 36. Loss: 0.360563725233078.\n",
            "Epoch: 37. Loss: 0.6617249846458435.\n",
            "Epoch: 38. Loss: 0.3634808361530304.\n",
            "Epoch: 39. Loss: 3.6405081748962402.\n",
            "Epoch: 40. Loss: 0.27164310216903687.\n",
            "Epoch: 41. Loss: 1.8751578330993652.\n",
            "Epoch: 42. Loss: 3.062624216079712.\n",
            "Epoch: 43. Loss: 3.2607061862945557.\n",
            "Epoch: 44. Loss: 0.27020925283432007.\n",
            "Epoch: 45. Loss: 2.373837471008301.\n",
            "Epoch: 46. Loss: 3.002207040786743.\n",
            "Epoch: 47. Loss: 0.21853360533714294.\n",
            "Epoch: 48. Loss: 1.3385401964187622.\n",
            "Epoch: 49. Loss: 1.1323481798171997.\n",
            "Epoch: 50. Loss: 0.4864112436771393.\n",
            "Epoch: 51. Loss: 3.529120683670044.\n",
            "Epoch: 52. Loss: 0.9281006455421448.\n",
            "Epoch: 53. Loss: 1.9315129518508911.\n",
            "Epoch: 54. Loss: 0.29456627368927.\n",
            "Epoch: 55. Loss: 2.249703884124756.\n",
            "Epoch: 56. Loss: 10.018754005432129.\n",
            "Epoch: 57. Loss: 0.8755030035972595.\n",
            "Epoch: 58. Loss: 1.2721635103225708.\n",
            "Epoch: 59. Loss: 0.6004233956336975.\n",
            "Epoch: 60. Loss: 2.210217237472534.\n",
            "Epoch: 61. Loss: 1.2252908945083618.\n",
            "Epoch: 62. Loss: 0.201809361577034.\n",
            "Epoch: 63. Loss: 0.8350740671157837.\n",
            "Epoch: 64. Loss: 1.3829749822616577.\n",
            "Epoch: 65. Loss: 0.8240323662757874.\n",
            "Epoch: 66. Loss: 0.45709505677223206.\n",
            "Epoch: 67. Loss: 0.247937873005867.\n",
            "Epoch: 68. Loss: 0.4579738676548004.\n",
            "Epoch: 69. Loss: 0.7197220325469971.\n",
            "Epoch: 70. Loss: 0.12242743372917175.\n",
            "Epoch: 71. Loss: 0.08964045345783234.\n",
            "Epoch: 72. Loss: 0.3108566105365753.\n",
            "Epoch: 73. Loss: 0.20766843855381012.\n",
            "Epoch: 74. Loss: 0.31388571858406067.\n",
            "Epoch: 75. Loss: 1.2206987142562866.\n",
            "Epoch: 76. Loss: 0.39028099179267883.\n",
            "Epoch: 77. Loss: 0.23601756989955902.\n",
            "Epoch: 78. Loss: 2.3619484901428223.\n",
            "Epoch: 79. Loss: 0.48565617203712463.\n",
            "Epoch: 80. Loss: 0.7656307816505432.\n",
            "Epoch: 81. Loss: 2.3421993255615234.\n",
            "Epoch: 82. Loss: 0.2468106597661972.\n",
            "Epoch: 83. Loss: 2.152747869491577.\n",
            "Epoch: 84. Loss: 0.2507328391075134.\n",
            "Epoch: 85. Loss: 0.14726951718330383.\n",
            "Epoch: 86. Loss: 0.33362698554992676.\n",
            "Epoch: 87. Loss: 0.275198370218277.\n",
            "Epoch: 88. Loss: 0.24198026955127716.\n",
            "Epoch: 89. Loss: 0.3622959554195404.\n",
            "Epoch: 90. Loss: 0.10639098286628723.\n",
            "Epoch: 91. Loss: 0.4266962707042694.\n",
            "Epoch: 92. Loss: 0.1357705146074295.\n",
            "Epoch: 93. Loss: 0.5344013571739197.\n",
            "Epoch: 94. Loss: 0.42121803760528564.\n",
            "Epoch: 95. Loss: 0.16375866532325745.\n",
            "Epoch: 96. Loss: 0.8653414249420166.\n",
            "Epoch: 97. Loss: 0.3868354558944702.\n",
            "Epoch: 98. Loss: 0.16270117461681366.\n",
            "Epoch: 99. Loss: 0.09318814426660538.\n",
            "Epoch: 100. Loss: 0.1977723091840744.\n",
            "Epoch: 101. Loss: 0.6282888650894165.\n",
            "Epoch: 102. Loss: 0.779125452041626.\n",
            "Epoch: 103. Loss: 0.6132653951644897.\n",
            "Epoch: 104. Loss: 0.22751739621162415.\n",
            "Epoch: 105. Loss: 0.4662895202636719.\n",
            "Epoch: 106. Loss: 0.4276767671108246.\n",
            "Epoch: 107. Loss: 0.2576126158237457.\n",
            "Epoch: 108. Loss: 0.6909732222557068.\n",
            "Epoch: 109. Loss: 0.2516464591026306.\n",
            "Epoch: 110. Loss: 0.3501982092857361.\n",
            "Epoch: 111. Loss: 0.2316548228263855.\n",
            "Epoch: 112. Loss: 0.13949063420295715.\n",
            "Epoch: 113. Loss: 0.08900655061006546.\n",
            "Epoch: 114. Loss: 0.20384125411510468.\n",
            "Epoch: 115. Loss: 0.30932489037513733.\n",
            "Epoch: 116. Loss: 0.3057337999343872.\n",
            "Epoch: 117. Loss: 0.10307295620441437.\n",
            "Epoch: 118. Loss: 0.33358752727508545.\n",
            "Epoch: 119. Loss: 1.3917886018753052.\n",
            "Epoch: 120. Loss: 0.232460618019104.\n",
            "Epoch: 121. Loss: 0.49651309847831726.\n",
            "Epoch: 122. Loss: 0.3116881251335144.\n",
            "Epoch: 123. Loss: 0.16937527060508728.\n",
            "Epoch: 124. Loss: 0.3366575241088867.\n",
            "Epoch: 125. Loss: 0.35759586095809937.\n",
            "Epoch: 126. Loss: 0.3839944005012512.\n",
            "Epoch: 127. Loss: 0.6943934559822083.\n",
            "Epoch: 128. Loss: 0.19579358398914337.\n",
            "Epoch: 129. Loss: 0.2094276398420334.\n",
            "Epoch: 130. Loss: 0.07119232416152954.\n",
            "Epoch: 131. Loss: 0.2860415577888489.\n",
            "Epoch: 132. Loss: 0.7972192168235779.\n",
            "Epoch: 133. Loss: 0.1135968267917633.\n",
            "Epoch: 134. Loss: 0.4579285681247711.\n",
            "Epoch: 135. Loss: 0.22299247980117798.\n",
            "Epoch: 136. Loss: 0.07708435505628586.\n",
            "Epoch: 137. Loss: 0.24639111757278442.\n",
            "Epoch: 138. Loss: 0.24558453261852264.\n",
            "Epoch: 139. Loss: 0.2542473375797272.\n",
            "Epoch: 140. Loss: 0.16903802752494812.\n",
            "Epoch: 141. Loss: 1.031511664390564.\n",
            "Epoch: 142. Loss: 0.28252822160720825.\n",
            "Epoch: 143. Loss: 0.09869987517595291.\n",
            "Epoch: 144. Loss: 0.17866721749305725.\n",
            "Epoch: 145. Loss: 0.06709206849336624.\n",
            "Epoch: 146. Loss: 0.23968513309955597.\n",
            "Epoch: 147. Loss: 0.12862348556518555.\n",
            "Epoch: 148. Loss: 0.5632187128067017.\n",
            "Epoch: 149. Loss: 0.21491961181163788.\n",
            "Epoch: 150. Loss: 0.1869942992925644.\n",
            "Epoch: 151. Loss: 0.402904748916626.\n",
            "Epoch: 152. Loss: 0.30904826521873474.\n",
            "Epoch: 153. Loss: 0.061349719762802124.\n",
            "Epoch: 154. Loss: 0.0497637540102005.\n",
            "Epoch: 155. Loss: 0.1476912647485733.\n",
            "Epoch: 156. Loss: 0.15515446662902832.\n",
            "Epoch: 157. Loss: 0.10654117912054062.\n",
            "Epoch: 158. Loss: 0.11065701395273209.\n",
            "Epoch: 159. Loss: 0.13666106760501862.\n",
            "Epoch: 160. Loss: 0.34165650606155396.\n",
            "Epoch: 161. Loss: 0.15201601386070251.\n",
            "Epoch: 162. Loss: 0.18335555493831635.\n",
            "Epoch: 163. Loss: 0.8308696150779724.\n",
            "Epoch: 164. Loss: 0.337899774312973.\n",
            "Epoch: 165. Loss: 0.2653324604034424.\n",
            "Epoch: 166. Loss: 0.1215890422463417.\n",
            "Epoch: 167. Loss: 0.14561885595321655.\n",
            "Epoch: 168. Loss: 0.25205162167549133.\n",
            "Epoch: 169. Loss: 0.11455188691616058.\n",
            "Epoch: 170. Loss: 0.3435245156288147.\n",
            "Epoch: 171. Loss: 0.08301721513271332.\n",
            "Epoch: 172. Loss: 0.0905391275882721.\n",
            "Epoch: 173. Loss: 0.31626829504966736.\n",
            "Epoch: 174. Loss: 0.3359985053539276.\n",
            "Epoch: 175. Loss: 0.20840604603290558.\n",
            "Epoch: 176. Loss: 0.2525365948677063.\n",
            "Epoch: 177. Loss: 0.29771357774734497.\n",
            "Epoch: 178. Loss: 0.6411334872245789.\n",
            "Epoch: 179. Loss: 0.48943203687667847.\n",
            "Epoch: 180. Loss: 0.2010686695575714.\n",
            "Epoch: 181. Loss: 0.19348376989364624.\n",
            "Epoch: 182. Loss: 0.05624527484178543.\n",
            "Epoch: 183. Loss: 0.2371470034122467.\n",
            "Epoch: 184. Loss: 0.15899981558322906.\n",
            "Epoch: 185. Loss: 0.18773765861988068.\n",
            "Epoch: 186. Loss: 0.0980566218495369.\n",
            "Epoch: 187. Loss: 0.07297884672880173.\n",
            "Epoch: 188. Loss: 0.5537267327308655.\n",
            "Epoch: 189. Loss: 0.19766943156719208.\n",
            "Epoch: 190. Loss: 0.22111086547374725.\n",
            "Epoch: 191. Loss: 0.26056185364723206.\n",
            "Epoch: 192. Loss: 0.4691636264324188.\n",
            "Epoch: 193. Loss: 0.12186221033334732.\n",
            "Epoch: 194. Loss: 0.18267636001110077.\n",
            "Epoch: 195. Loss: 0.2263970822095871.\n",
            "Epoch: 196. Loss: 0.2057235836982727.\n",
            "Epoch: 197. Loss: 0.08620493113994598.\n",
            "Epoch: 198. Loss: 0.05000085383653641.\n",
            "Epoch: 199. Loss: 0.12286362797021866.\n",
            "Epoch: 200. Loss: 0.39116790890693665.\n",
            "Epoch: 201. Loss: 0.31224095821380615.\n",
            "Epoch: 202. Loss: 0.13220955431461334.\n",
            "Epoch: 203. Loss: 0.12420695275068283.\n",
            "Epoch: 204. Loss: 0.09800941497087479.\n",
            "Epoch: 205. Loss: 0.3357846438884735.\n",
            "Epoch: 206. Loss: 0.18372595310211182.\n",
            "Epoch: 207. Loss: 0.08525435626506805.\n",
            "Epoch: 208. Loss: 0.1251761019229889.\n",
            "Epoch: 209. Loss: 0.17206820845603943.\n",
            "Epoch: 210. Loss: 0.08275409787893295.\n",
            "Epoch: 211. Loss: 0.291542649269104.\n",
            "Epoch: 212. Loss: 0.09122242033481598.\n",
            "Epoch: 213. Loss: 0.2970800995826721.\n",
            "Epoch: 214. Loss: 0.24882008135318756.\n",
            "Epoch: 215. Loss: 0.24123656749725342.\n",
            "Epoch: 216. Loss: 0.40780770778656006.\n",
            "Epoch: 217. Loss: 0.27164021134376526.\n",
            "Epoch: 218. Loss: 0.5324202179908752.\n",
            "Epoch: 219. Loss: 0.3897266387939453.\n",
            "Epoch: 220. Loss: 0.4515910744667053.\n",
            "Epoch: 221. Loss: 0.15988390147686005.\n",
            "Epoch: 222. Loss: 0.13825756311416626.\n",
            "Epoch: 223. Loss: 0.14559127390384674.\n",
            "Epoch: 224. Loss: 0.23181085288524628.\n",
            "Epoch: 225. Loss: 0.29027095437049866.\n",
            "Epoch: 226. Loss: 0.12428314238786697.\n",
            "Epoch: 227. Loss: 0.07467086613178253.\n",
            "Epoch: 228. Loss: 0.17482149600982666.\n",
            "Epoch: 229. Loss: 0.49223652482032776.\n",
            "Epoch: 230. Loss: 0.23599903285503387.\n",
            "Epoch: 231. Loss: 0.210377037525177.\n",
            "Epoch: 232. Loss: 0.22061128914356232.\n",
            "Epoch: 233. Loss: 0.428385466337204.\n",
            "Epoch: 234. Loss: 0.0940251424908638.\n",
            "Epoch: 235. Loss: 0.054568804800510406.\n",
            "Epoch: 236. Loss: 0.3858672082424164.\n",
            "Epoch: 237. Loss: 0.6759599447250366.\n",
            "Epoch: 238. Loss: 0.10068847984075546.\n",
            "Epoch: 239. Loss: 0.24005109071731567.\n",
            "Epoch: 240. Loss: 0.4698816239833832.\n",
            "Epoch: 241. Loss: 0.6923991441726685.\n",
            "Epoch: 242. Loss: 0.1356717050075531.\n",
            "Epoch: 243. Loss: 0.28476011753082275.\n",
            "Epoch: 244. Loss: 0.42378783226013184.\n",
            "Epoch: 245. Loss: 0.129106804728508.\n",
            "Epoch: 246. Loss: 0.19093289971351624.\n",
            "Epoch: 247. Loss: 0.6343359351158142.\n",
            "Epoch: 248. Loss: 0.09812507778406143.\n",
            "Epoch: 249. Loss: 0.12357370555400848.\n",
            "Epoch: 250. Loss: 0.17854228615760803.\n",
            "Epoch: 251. Loss: 0.18257129192352295.\n",
            "Epoch: 252. Loss: 0.21788237988948822.\n",
            "Epoch: 253. Loss: 0.15327371656894684.\n",
            "Epoch: 254. Loss: 0.2122875154018402.\n",
            "Epoch: 255. Loss: 0.19223619997501373.\n",
            "Epoch: 256. Loss: 0.18274088203907013.\n",
            "Epoch: 257. Loss: 0.18956802785396576.\n",
            "Epoch: 258. Loss: 0.7923094034194946.\n",
            "Epoch: 259. Loss: 0.04437416419386864.\n",
            "Epoch: 260. Loss: 0.15695996582508087.\n",
            "Epoch: 261. Loss: 0.037731945514678955.\n",
            "Epoch: 262. Loss: 0.04854794591665268.\n",
            "Epoch: 263. Loss: 0.4292527735233307.\n",
            "Epoch: 264. Loss: 0.07576833665370941.\n",
            "Epoch: 265. Loss: 0.1706395298242569.\n",
            "Epoch: 266. Loss: 0.1022711768746376.\n",
            "Epoch: 267. Loss: 0.3174840211868286.\n",
            "Epoch: 268. Loss: 0.30235928297042847.\n",
            "Epoch: 269. Loss: 0.27927666902542114.\n",
            "Epoch: 270. Loss: 0.2848133444786072.\n",
            "Epoch: 271. Loss: 0.2177302986383438.\n",
            "Epoch: 272. Loss: 0.10958977043628693.\n",
            "Epoch: 273. Loss: 0.10958392173051834.\n",
            "Epoch: 274. Loss: 0.13810089230537415.\n",
            "Epoch: 275. Loss: 0.06606056541204453.\n",
            "Epoch: 276. Loss: 0.030798356980085373.\n",
            "Epoch: 277. Loss: 0.30930066108703613.\n",
            "Epoch: 278. Loss: 0.40359288454055786.\n",
            "Epoch: 279. Loss: 0.11166027933359146.\n",
            "Epoch: 280. Loss: 0.22397172451019287.\n",
            "Epoch: 281. Loss: 0.7303369045257568.\n",
            "Epoch: 282. Loss: 0.1986701935529709.\n",
            "Epoch: 283. Loss: 0.07347624003887177.\n",
            "Epoch: 284. Loss: 0.11173108220100403.\n",
            "Epoch: 285. Loss: 0.3093084990978241.\n",
            "Epoch: 286. Loss: 0.1005568876862526.\n",
            "Epoch: 287. Loss: 0.4368295669555664.\n",
            "Epoch: 288. Loss: 0.1767858862876892.\n",
            "Epoch: 289. Loss: 0.1767481416463852.\n",
            "Epoch: 290. Loss: 0.14951160550117493.\n",
            "Epoch: 291. Loss: 0.1271028071641922.\n",
            "Epoch: 292. Loss: 0.18129459023475647.\n",
            "Epoch: 293. Loss: 0.07814466953277588.\n",
            "Epoch: 294. Loss: 0.17854413390159607.\n",
            "Epoch: 295. Loss: 0.6072005033493042.\n",
            "Epoch: 296. Loss: 0.12229517102241516.\n",
            "Epoch: 297. Loss: 0.29119277000427246.\n",
            "Epoch: 298. Loss: 0.21675905585289001.\n",
            "Epoch: 299. Loss: 0.3049534857273102.\n",
            "Epoch: 300. Loss: 0.26268675923347473.\n",
            "Epoch: 301. Loss: 0.16834896802902222.\n",
            "Epoch: 302. Loss: 0.3381628096103668.\n",
            "Epoch: 303. Loss: 0.17179426550865173.\n",
            "Epoch: 304. Loss: 0.24526181817054749.\n",
            "Epoch: 305. Loss: 0.20623184740543365.\n",
            "Epoch: 306. Loss: 0.09396146982908249.\n",
            "Epoch: 307. Loss: 0.1318223774433136.\n",
            "Epoch: 308. Loss: 0.03184927627444267.\n",
            "Epoch: 309. Loss: 0.14672470092773438.\n",
            "Epoch: 310. Loss: 0.652413010597229.\n",
            "Epoch: 311. Loss: 0.12005938589572906.\n",
            "Epoch: 312. Loss: 0.09914058446884155.\n",
            "Epoch: 313. Loss: 0.11755680292844772.\n",
            "Epoch: 314. Loss: 0.13176371157169342.\n",
            "Epoch: 315. Loss: 0.49784985184669495.\n",
            "Epoch: 316. Loss: 0.05675698444247246.\n",
            "Epoch: 317. Loss: 0.16272367537021637.\n",
            "Epoch: 318. Loss: 0.3297621011734009.\n",
            "Epoch: 319. Loss: 0.08605153858661652.\n",
            "Epoch: 320. Loss: 0.08170730620622635.\n",
            "Epoch: 321. Loss: 0.17887592315673828.\n",
            "Epoch: 322. Loss: 0.11276797950267792.\n",
            "Epoch: 323. Loss: 0.25460126996040344.\n",
            "Epoch: 324. Loss: 0.27160605788230896.\n",
            "Epoch: 325. Loss: 0.14418095350265503.\n",
            "Epoch: 326. Loss: 0.1448364108800888.\n",
            "Epoch: 327. Loss: 0.0800994336605072.\n",
            "Epoch: 328. Loss: 0.12832732498645782.\n",
            "Epoch: 329. Loss: 0.13199014961719513.\n",
            "Epoch: 330. Loss: 0.028578968718647957.\n",
            "Epoch: 331. Loss: 0.24029628932476044.\n",
            "Epoch: 332. Loss: 0.22749513387680054.\n",
            "Epoch: 333. Loss: 0.07763537764549255.\n",
            "Epoch: 334. Loss: 0.10694432258605957.\n",
            "Epoch: 335. Loss: 0.10704836249351501.\n",
            "Epoch: 336. Loss: 0.12115045636892319.\n",
            "Epoch: 337. Loss: 0.10614774376153946.\n",
            "Epoch: 338. Loss: 0.1701197475194931.\n",
            "Epoch: 339. Loss: 0.13078807294368744.\n",
            "Epoch: 340. Loss: 0.24483369290828705.\n",
            "Epoch: 341. Loss: 0.18050691485404968.\n",
            "Epoch: 342. Loss: 0.1778462529182434.\n",
            "Epoch: 343. Loss: 0.1769750714302063.\n",
            "Epoch: 344. Loss: 0.22321921586990356.\n",
            "Epoch: 345. Loss: 0.1891094148159027.\n",
            "Epoch: 346. Loss: 0.12493487447500229.\n",
            "Epoch: 347. Loss: 0.14606310427188873.\n",
            "Epoch: 348. Loss: 0.2918195426464081.\n",
            "Epoch: 349. Loss: 0.0438925065100193.\n",
            "Epoch: 350. Loss: 0.13362334668636322.\n",
            "Epoch: 351. Loss: 0.2515566945075989.\n",
            "Epoch: 352. Loss: 0.12972742319107056.\n",
            "Epoch: 353. Loss: 0.1710710972547531.\n",
            "Epoch: 354. Loss: 0.1438012719154358.\n",
            "Epoch: 355. Loss: 0.10610876232385635.\n",
            "Epoch: 356. Loss: 0.16758207976818085.\n",
            "Epoch: 357. Loss: 0.12227435410022736.\n",
            "Epoch: 358. Loss: 0.0892723873257637.\n",
            "Epoch: 359. Loss: 0.41026797890663147.\n",
            "Epoch: 360. Loss: 0.14300018548965454.\n",
            "Epoch: 361. Loss: 0.1929706186056137.\n",
            "Epoch: 362. Loss: 0.20278200507164001.\n",
            "Epoch: 363. Loss: 0.08190564811229706.\n",
            "Epoch: 364. Loss: 0.20635578036308289.\n",
            "Epoch: 365. Loss: 0.26713159680366516.\n",
            "Epoch: 366. Loss: 0.13798780739307404.\n",
            "Epoch: 367. Loss: 0.30241817235946655.\n",
            "Epoch: 368. Loss: 0.14530254900455475.\n",
            "Epoch: 369. Loss: 0.17944912612438202.\n",
            "Epoch: 370. Loss: 0.12225919961929321.\n",
            "Epoch: 371. Loss: 0.1997031718492508.\n",
            "Epoch: 372. Loss: 0.20866671204566956.\n",
            "Epoch: 373. Loss: 0.2501479983329773.\n",
            "Epoch: 374. Loss: 0.18092232942581177.\n",
            "Epoch: 375. Loss: 0.2857091426849365.\n",
            "Epoch: 376. Loss: 0.1124291643500328.\n",
            "Epoch: 377. Loss: 0.18621617555618286.\n",
            "Epoch: 378. Loss: 0.21129845082759857.\n",
            "Epoch: 379. Loss: 0.14528578519821167.\n",
            "Epoch: 380. Loss: 0.24081139266490936.\n",
            "Epoch: 381. Loss: 0.662804126739502.\n",
            "Epoch: 382. Loss: 0.05952638015151024.\n",
            "Epoch: 383. Loss: 0.22940261662006378.\n",
            "Epoch: 384. Loss: 0.1434207260608673.\n",
            "Epoch: 385. Loss: 0.18173103034496307.\n",
            "Epoch: 386. Loss: 0.28032955527305603.\n",
            "Epoch: 387. Loss: 0.13635677099227905.\n",
            "Epoch: 388. Loss: 0.09786302596330643.\n",
            "Epoch: 389. Loss: 0.20328235626220703.\n",
            "Epoch: 390. Loss: 0.19628062844276428.\n",
            "Epoch: 391. Loss: 0.09696084260940552.\n",
            "Epoch: 392. Loss: 0.18800058960914612.\n",
            "Epoch: 393. Loss: 0.28943631052970886.\n",
            "Epoch: 394. Loss: 0.22159308195114136.\n",
            "Epoch: 395. Loss: 0.15256215631961823.\n",
            "Epoch: 396. Loss: 0.16568617522716522.\n",
            "Epoch: 397. Loss: 0.21594421565532684.\n",
            "Epoch: 398. Loss: 0.3133525550365448.\n",
            "Epoch: 399. Loss: 0.17131894826889038.\n",
            "Epoch: 400. Loss: 0.2599673271179199.\n",
            "Epoch: 401. Loss: 0.09313515573740005.\n",
            "Epoch: 402. Loss: 0.14249391853809357.\n",
            "Epoch: 403. Loss: 0.18878322839736938.\n",
            "Epoch: 404. Loss: 0.27168354392051697.\n",
            "Epoch: 405. Loss: 0.09692910313606262.\n",
            "Epoch: 406. Loss: 0.1918666660785675.\n",
            "Epoch: 407. Loss: 0.22278480231761932.\n",
            "Epoch: 408. Loss: 0.03507351502776146.\n",
            "Epoch: 409. Loss: 0.029930785298347473.\n",
            "Epoch: 410. Loss: 0.12198272347450256.\n",
            "Epoch: 411. Loss: 0.22582393884658813.\n",
            "Epoch: 412. Loss: 0.18723896145820618.\n",
            "Epoch: 413. Loss: 0.3446750342845917.\n",
            "Epoch: 414. Loss: 0.2280941605567932.\n",
            "Epoch: 415. Loss: 0.10027269273996353.\n",
            "Epoch: 416. Loss: 0.2007717490196228.\n",
            "Epoch: 417. Loss: 0.16093891859054565.\n",
            "Epoch: 418. Loss: 0.21276868879795074.\n",
            "Epoch: 419. Loss: 0.25778576731681824.\n",
            "Epoch: 420. Loss: 0.17996808886528015.\n",
            "Epoch: 421. Loss: 0.07719197869300842.\n",
            "Epoch: 422. Loss: 0.1643107533454895.\n",
            "Epoch: 423. Loss: 0.3091115355491638.\n",
            "Epoch: 424. Loss: 0.1766958236694336.\n",
            "Epoch: 425. Loss: 0.12111334502696991.\n",
            "Epoch: 426. Loss: 0.17699745297431946.\n",
            "Epoch: 427. Loss: 0.07850641012191772.\n",
            "Epoch: 428. Loss: 0.19387413561344147.\n",
            "Epoch: 429. Loss: 0.14858165383338928.\n",
            "Epoch: 430. Loss: 0.32606393098831177.\n",
            "Epoch: 431. Loss: 0.20988653600215912.\n",
            "Epoch: 432. Loss: 0.10179205238819122.\n",
            "Epoch: 433. Loss: 0.35028931498527527.\n",
            "Epoch: 434. Loss: 0.07565687596797943.\n",
            "Epoch: 435. Loss: 0.14075805246829987.\n",
            "Epoch: 436. Loss: 0.2628568410873413.\n",
            "Epoch: 437. Loss: 0.22887930274009705.\n",
            "Epoch: 438. Loss: 0.14933905005455017.\n",
            "Epoch: 439. Loss: 0.1232687383890152.\n",
            "Epoch: 440. Loss: 0.12359868735074997.\n",
            "Epoch: 441. Loss: 0.06774675846099854.\n",
            "Epoch: 442. Loss: 0.06135799363255501.\n",
            "Epoch: 443. Loss: 0.1585356742143631.\n",
            "Epoch: 444. Loss: 0.07467465102672577.\n",
            "Epoch: 445. Loss: 0.21259962022304535.\n",
            "Epoch: 446. Loss: 0.03137562423944473.\n",
            "Epoch: 447. Loss: 0.2823614776134491.\n",
            "Epoch: 448. Loss: 0.08085542917251587.\n",
            "Epoch: 449. Loss: 0.0970943421125412.\n",
            "Epoch: 450. Loss: 0.08503220230340958.\n",
            "Epoch: 451. Loss: 0.08473517745733261.\n",
            "Epoch: 452. Loss: 0.0837380588054657.\n",
            "Epoch: 453. Loss: 0.17226868867874146.\n",
            "Epoch: 454. Loss: 0.10192155092954636.\n",
            "Epoch: 455. Loss: 0.22193580865859985.\n",
            "Epoch: 456. Loss: 0.14397992193698883.\n",
            "Epoch: 457. Loss: 0.06068379804491997.\n",
            "Epoch: 458. Loss: 0.11834201961755753.\n",
            "Epoch: 459. Loss: 0.20161081850528717.\n",
            "Epoch: 460. Loss: 0.11678897589445114.\n",
            "Epoch: 461. Loss: 0.15345551073551178.\n",
            "Epoch: 462. Loss: 0.07420118153095245.\n",
            "Epoch: 463. Loss: 0.2458210587501526.\n",
            "Epoch: 464. Loss: 0.17373254895210266.\n",
            "Epoch: 465. Loss: 0.09197007864713669.\n",
            "Epoch: 466. Loss: 0.11798816919326782.\n",
            "Epoch: 467. Loss: 0.10735370963811874.\n",
            "Epoch: 468. Loss: 0.12720328569412231.\n",
            "Epoch: 469. Loss: 0.24987180531024933.\n",
            "Epoch: 470. Loss: 0.1218673437833786.\n",
            "Epoch: 471. Loss: 0.06802906841039658.\n",
            "Epoch: 472. Loss: 0.10249659419059753.\n",
            "Epoch: 473. Loss: 0.13098277151584625.\n",
            "Epoch: 474. Loss: 0.17460384964942932.\n",
            "Epoch: 475. Loss: 0.1310449093580246.\n",
            "Epoch: 476. Loss: 0.030835585668683052.\n",
            "Epoch: 477. Loss: 0.1487521529197693.\n",
            "Epoch: 478. Loss: 0.14794772863388062.\n",
            "Epoch: 479. Loss: 0.03760803863406181.\n",
            "Epoch: 480. Loss: 0.30346667766571045.\n",
            "Epoch: 481. Loss: 0.17111210525035858.\n",
            "Epoch: 482. Loss: 0.08516573160886765.\n",
            "Epoch: 483. Loss: 0.15322451293468475.\n",
            "Epoch: 484. Loss: 0.12760049104690552.\n",
            "Epoch: 485. Loss: 0.10699867457151413.\n",
            "Epoch: 486. Loss: 0.16184043884277344.\n",
            "Epoch: 487. Loss: 0.17866459488868713.\n",
            "Epoch: 488. Loss: 0.18134388327598572.\n",
            "Epoch: 489. Loss: 0.16537530720233917.\n",
            "Epoch: 490. Loss: 0.09663400053977966.\n",
            "Epoch: 491. Loss: 0.09504860639572144.\n",
            "Epoch: 492. Loss: 0.13615506887435913.\n",
            "Epoch: 493. Loss: 0.22323425114154816.\n",
            "Epoch: 494. Loss: 0.1864752173423767.\n",
            "Epoch: 495. Loss: 0.3173952102661133.\n",
            "Epoch: 496. Loss: 0.251653790473938.\n",
            "Epoch: 497. Loss: 0.07397136837244034.\n",
            "Epoch: 498. Loss: 0.10485473275184631.\n",
            "Epoch: 499. Loss: 0.0385567732155323.\n",
            "Epoch: 500. Loss: 0.3518863916397095.\n",
            "Epoch: 501. Loss: 0.30599117279052734.\n",
            "Epoch: 502. Loss: 0.09779229760169983.\n",
            "Epoch: 503. Loss: 0.07074183970689774.\n",
            "Epoch: 504. Loss: 0.21010971069335938.\n",
            "Epoch: 505. Loss: 0.16389812529087067.\n",
            "Epoch: 506. Loss: 0.12440039962530136.\n",
            "Epoch: 507. Loss: 0.23129339516162872.\n",
            "Epoch: 508. Loss: 0.0942985787987709.\n",
            "Epoch: 509. Loss: 0.25665056705474854.\n",
            "Epoch: 510. Loss: 0.0319182313978672.\n",
            "Epoch: 511. Loss: 0.1208973303437233.\n",
            "Epoch: 512. Loss: 0.09645649045705795.\n",
            "Epoch: 513. Loss: 0.0710565447807312.\n",
            "Epoch: 514. Loss: 0.11635337024927139.\n",
            "Epoch: 515. Loss: 0.20766739547252655.\n",
            "Epoch: 516. Loss: 0.12644147872924805.\n",
            "Epoch: 517. Loss: 0.13327497243881226.\n",
            "Epoch: 518. Loss: 0.27920129895210266.\n",
            "Epoch: 519. Loss: 0.08433230221271515.\n",
            "Epoch: 520. Loss: 0.049880970269441605.\n",
            "Epoch: 521. Loss: 0.1503925919532776.\n",
            "Epoch: 522. Loss: 0.33272814750671387.\n",
            "Epoch: 523. Loss: 0.09804534167051315.\n",
            "Epoch: 524. Loss: 0.12929971516132355.\n",
            "Epoch: 525. Loss: 0.10364533215761185.\n",
            "Epoch: 526. Loss: 0.12645162642002106.\n",
            "Epoch: 527. Loss: 0.11258786916732788.\n",
            "Epoch: 528. Loss: 0.13339142501354218.\n",
            "Epoch: 529. Loss: 0.16160331666469574.\n",
            "Epoch: 530. Loss: 0.12432858347892761.\n",
            "Epoch: 531. Loss: 0.06869116425514221.\n",
            "Epoch: 532. Loss: 0.16124889254570007.\n",
            "Epoch: 533. Loss: 0.02877986989915371.\n",
            "Epoch: 534. Loss: 0.17430992424488068.\n",
            "Epoch: 535. Loss: 0.22439827024936676.\n",
            "Epoch: 536. Loss: 0.09768316149711609.\n",
            "Epoch: 537. Loss: 0.132024347782135.\n",
            "Epoch: 538. Loss: 0.09109111875295639.\n",
            "Epoch: 539. Loss: 0.11680854856967926.\n",
            "Epoch: 540. Loss: 0.1642768532037735.\n",
            "Epoch: 541. Loss: 0.09639349579811096.\n",
            "Epoch: 542. Loss: 0.18899086117744446.\n",
            "Epoch: 543. Loss: 0.03194635733962059.\n",
            "Epoch: 544. Loss: 0.24901363253593445.\n",
            "Epoch: 545. Loss: 0.08719654381275177.\n",
            "Epoch: 546. Loss: 0.2643381357192993.\n",
            "Epoch: 547. Loss: 0.08451465517282486.\n",
            "Epoch: 548. Loss: 0.11388645321130753.\n",
            "Epoch: 549. Loss: 0.31559133529663086.\n",
            "Epoch: 550. Loss: 0.24868452548980713.\n",
            "Epoch: 551. Loss: 0.13399638235569.\n",
            "Epoch: 552. Loss: 0.30423828959465027.\n",
            "Epoch: 553. Loss: 0.11044182628393173.\n",
            "Epoch: 554. Loss: 0.3187633454799652.\n",
            "Epoch: 555. Loss: 0.3172193765640259.\n",
            "Epoch: 556. Loss: 0.03265727311372757.\n",
            "Epoch: 557. Loss: 0.26895514130592346.\n",
            "Epoch: 558. Loss: 0.07554822415113449.\n",
            "Epoch: 559. Loss: 0.1294364631175995.\n",
            "Epoch: 560. Loss: 0.17175982892513275.\n",
            "Epoch: 561. Loss: 0.2102121114730835.\n",
            "Epoch: 562. Loss: 0.14982782304286957.\n",
            "Epoch: 563. Loss: 0.16124162077903748.\n",
            "Epoch: 564. Loss: 0.1819709986448288.\n",
            "Epoch: 565. Loss: 0.15537695586681366.\n",
            "Epoch: 566. Loss: 0.16576749086380005.\n",
            "Epoch: 567. Loss: 0.3478116989135742.\n",
            "Epoch: 568. Loss: 0.2839857339859009.\n",
            "Epoch: 569. Loss: 0.04233301803469658.\n",
            "Epoch: 570. Loss: 0.25784674286842346.\n",
            "Epoch: 571. Loss: 0.15296238660812378.\n",
            "Epoch: 572. Loss: 0.08299902826547623.\n",
            "Epoch: 573. Loss: 0.09082141518592834.\n",
            "Epoch: 574. Loss: 0.06481793522834778.\n",
            "Epoch: 575. Loss: 0.17243045568466187.\n",
            "Epoch: 576. Loss: 0.19682905077934265.\n",
            "Epoch: 577. Loss: 0.09736187011003494.\n",
            "Epoch: 578. Loss: 0.11716417968273163.\n",
            "Epoch: 579. Loss: 0.2161083221435547.\n",
            "Epoch: 580. Loss: 0.10062262415885925.\n",
            "Epoch: 581. Loss: 0.2000613808631897.\n",
            "Epoch: 582. Loss: 0.0992514118552208.\n",
            "Epoch: 583. Loss: 0.1861024796962738.\n",
            "Epoch: 584. Loss: 0.029826268553733826.\n",
            "Epoch: 585. Loss: 0.04838406667113304.\n",
            "Epoch: 586. Loss: 0.16979871690273285.\n",
            "Epoch: 587. Loss: 0.09921999275684357.\n",
            "Epoch: 588. Loss: 0.09971806406974792.\n",
            "Epoch: 589. Loss: 0.12916463613510132.\n",
            "Epoch: 590. Loss: 0.13532106578350067.\n",
            "Epoch: 591. Loss: 0.06127277761697769.\n",
            "Epoch: 592. Loss: 0.15635666251182556.\n",
            "Epoch: 593. Loss: 0.062086667865514755.\n",
            "Epoch: 594. Loss: 0.12261880189180374.\n",
            "Epoch: 595. Loss: 0.04465249925851822.\n",
            "Epoch: 596. Loss: 0.04625360667705536.\n",
            "Epoch: 597. Loss: 0.14866061508655548.\n",
            "Epoch: 598. Loss: 0.09297265112400055.\n",
            "Epoch: 599. Loss: 0.049229126423597336.\n",
            "Epoch: 600. Loss: 0.06784787029027939.\n",
            "Epoch: 601. Loss: 0.11905689537525177.\n",
            "Epoch: 602. Loss: 0.16254787147045135.\n",
            "Epoch: 603. Loss: 0.5143263339996338.\n",
            "Epoch: 604. Loss: 0.17736613750457764.\n",
            "Epoch: 605. Loss: 0.23904430866241455.\n",
            "Epoch: 606. Loss: 0.159074604511261.\n",
            "Epoch: 607. Loss: 0.2243800014257431.\n",
            "Epoch: 608. Loss: 0.16113048791885376.\n",
            "Epoch: 609. Loss: 0.14814786612987518.\n",
            "Epoch: 610. Loss: 0.15534813702106476.\n",
            "Epoch: 611. Loss: 0.1842992603778839.\n",
            "Epoch: 612. Loss: 0.2656094431877136.\n",
            "Epoch: 613. Loss: 0.0678023099899292.\n",
            "Epoch: 614. Loss: 0.14546668529510498.\n",
            "Epoch: 615. Loss: 0.10604450106620789.\n",
            "Epoch: 616. Loss: 0.298593670129776.\n",
            "Epoch: 617. Loss: 0.0786207839846611.\n",
            "Epoch: 618. Loss: 0.1987704485654831.\n",
            "Epoch: 619. Loss: 0.4365365207195282.\n",
            "Epoch: 620. Loss: 0.17631490528583527.\n",
            "Epoch: 621. Loss: 0.1771998554468155.\n",
            "Epoch: 622. Loss: 0.09411349147558212.\n",
            "Epoch: 623. Loss: 0.05926213413476944.\n",
            "Epoch: 624. Loss: 0.15303729474544525.\n",
            "Epoch: 625. Loss: 0.044941242784261703.\n",
            "Epoch: 626. Loss: 0.2327975630760193.\n",
            "Epoch: 627. Loss: 0.20518630743026733.\n",
            "Epoch: 628. Loss: 0.07876551896333694.\n",
            "Epoch: 629. Loss: 0.23230291903018951.\n",
            "Epoch: 630. Loss: 0.2333487868309021.\n",
            "Epoch: 631. Loss: 0.15038470923900604.\n",
            "Epoch: 632. Loss: 0.06900575011968613.\n",
            "Epoch: 633. Loss: 0.12359312921762466.\n",
            "Epoch: 634. Loss: 0.14586670696735382.\n",
            "Epoch: 635. Loss: 0.22823666036128998.\n",
            "Epoch: 636. Loss: 0.0401713103055954.\n",
            "Epoch: 637. Loss: 0.0727405920624733.\n",
            "Epoch: 638. Loss: 0.09059081971645355.\n",
            "Epoch: 639. Loss: 0.0900520607829094.\n",
            "Epoch: 640. Loss: 0.1644783467054367.\n",
            "Epoch: 641. Loss: 0.08991285413503647.\n",
            "Epoch: 642. Loss: 0.23337750136852264.\n",
            "Epoch: 643. Loss: 0.055509358644485474.\n",
            "Epoch: 644. Loss: 0.06373827904462814.\n",
            "Epoch: 645. Loss: 0.1981814205646515.\n",
            "Epoch: 646. Loss: 0.15828543901443481.\n",
            "Epoch: 647. Loss: 0.11630255728960037.\n",
            "Epoch: 648. Loss: 0.1566939502954483.\n",
            "Epoch: 649. Loss: 0.06596793979406357.\n",
            "Epoch: 650. Loss: 0.1580452024936676.\n",
            "Epoch: 651. Loss: 0.1716761440038681.\n",
            "Epoch: 652. Loss: 0.12200028449296951.\n",
            "Epoch: 653. Loss: 0.22608967125415802.\n",
            "Epoch: 654. Loss: 0.12953898310661316.\n",
            "Epoch: 655. Loss: 0.20865276455879211.\n",
            "Epoch: 656. Loss: 0.09400296956300735.\n",
            "Epoch: 657. Loss: 0.13010458648204803.\n",
            "Epoch: 658. Loss: 0.0821739137172699.\n",
            "Epoch: 659. Loss: 0.14647641777992249.\n",
            "Epoch: 660. Loss: 0.17453201115131378.\n",
            "Epoch: 661. Loss: 0.3092569410800934.\n",
            "Epoch: 662. Loss: 0.10630253702402115.\n",
            "Epoch: 663. Loss: 0.18767119944095612.\n",
            "Epoch: 664. Loss: 0.11276856064796448.\n",
            "Epoch: 665. Loss: 0.12033429741859436.\n",
            "Epoch: 666. Loss: 0.23717576265335083.\n",
            "Epoch: 667. Loss: 0.045851320028305054.\n",
            "Epoch: 668. Loss: 0.2576051950454712.\n",
            "Epoch: 669. Loss: 0.198203444480896.\n",
            "Epoch: 670. Loss: 0.22999674081802368.\n",
            "Epoch: 671. Loss: 0.08208286017179489.\n",
            "Epoch: 672. Loss: 0.08525965362787247.\n",
            "Epoch: 673. Loss: 0.11418437957763672.\n",
            "Epoch: 674. Loss: 0.19211852550506592.\n",
            "Epoch: 675. Loss: 0.2477867752313614.\n",
            "Epoch: 676. Loss: 0.1910046935081482.\n",
            "Epoch: 677. Loss: 0.04381796717643738.\n",
            "Epoch: 678. Loss: 0.15667127072811127.\n",
            "Epoch: 679. Loss: 0.1713723987340927.\n",
            "Epoch: 680. Loss: 0.06039491295814514.\n",
            "Epoch: 681. Loss: 0.09015709161758423.\n",
            "Epoch: 682. Loss: 0.12219882756471634.\n",
            "Epoch: 683. Loss: 0.19184482097625732.\n",
            "Epoch: 684. Loss: 0.19492767751216888.\n",
            "Epoch: 685. Loss: 0.12745466828346252.\n",
            "Epoch: 686. Loss: 0.10400345921516418.\n",
            "Epoch: 687. Loss: 0.11556782573461533.\n",
            "Epoch: 688. Loss: 0.13189609348773956.\n",
            "Epoch: 689. Loss: 0.3876095712184906.\n",
            "Epoch: 690. Loss: 0.21660783886909485.\n",
            "Epoch: 691. Loss: 0.0352039635181427.\n",
            "Epoch: 692. Loss: 0.08832862228155136.\n",
            "Epoch: 693. Loss: 0.09565863758325577.\n",
            "Epoch: 694. Loss: 0.128007709980011.\n",
            "Epoch: 695. Loss: 0.10802812874317169.\n",
            "Epoch: 696. Loss: 0.01602727174758911.\n",
            "Epoch: 697. Loss: 0.0695049837231636.\n",
            "Epoch: 698. Loss: 0.07724590599536896.\n",
            "Epoch: 699. Loss: 0.1164955347776413.\n",
            "Epoch: 700. Loss: 0.14768394827842712.\n",
            "Epoch: 701. Loss: 0.49492087960243225.\n",
            "Epoch: 702. Loss: 0.04728301987051964.\n",
            "Epoch: 703. Loss: 0.15738119184970856.\n",
            "Epoch: 704. Loss: 0.09937858581542969.\n",
            "Epoch: 705. Loss: 0.028516240417957306.\n",
            "Epoch: 706. Loss: 0.10391590744256973.\n",
            "Epoch: 707. Loss: 0.14136166870594025.\n",
            "Epoch: 708. Loss: 0.10323050618171692.\n",
            "Epoch: 709. Loss: 0.10940380394458771.\n",
            "Epoch: 710. Loss: 0.07659699767827988.\n",
            "Epoch: 711. Loss: 0.22732913494110107.\n",
            "Epoch: 712. Loss: 0.13662688434123993.\n",
            "Epoch: 713. Loss: 0.1584658920764923.\n",
            "Epoch: 714. Loss: 0.09044346958398819.\n",
            "Epoch: 715. Loss: 0.07136692106723785.\n",
            "Epoch: 716. Loss: 0.3639640808105469.\n",
            "Epoch: 717. Loss: 0.18567362427711487.\n",
            "Epoch: 718. Loss: 0.10815966874361038.\n",
            "Epoch: 719. Loss: 0.10816877335309982.\n",
            "Epoch: 720. Loss: 0.11833791434764862.\n",
            "Epoch: 721. Loss: 0.2514156997203827.\n",
            "Epoch: 722. Loss: 0.31593799591064453.\n",
            "Epoch: 723. Loss: 0.7048770189285278.\n",
            "Epoch: 724. Loss: 0.19304345548152924.\n",
            "Epoch: 725. Loss: 0.3537154495716095.\n",
            "Epoch: 726. Loss: 0.16301025450229645.\n",
            "Epoch: 727. Loss: 0.15189985930919647.\n",
            "Epoch: 728. Loss: 0.30190202593803406.\n",
            "Epoch: 729. Loss: 0.2750576138496399.\n",
            "Epoch: 730. Loss: 0.11300753802061081.\n",
            "Epoch: 731. Loss: 0.17834487557411194.\n",
            "Epoch: 732. Loss: 0.10529139637947083.\n",
            "Epoch: 733. Loss: 0.15844640135765076.\n",
            "Epoch: 734. Loss: 0.12972265481948853.\n",
            "Epoch: 735. Loss: 0.11644519120454788.\n",
            "Epoch: 736. Loss: 0.07362891733646393.\n",
            "Epoch: 737. Loss: 0.109233558177948.\n",
            "Epoch: 738. Loss: 0.07644739747047424.\n",
            "Epoch: 739. Loss: 0.12160137295722961.\n",
            "Epoch: 740. Loss: 0.2342328578233719.\n",
            "Epoch: 741. Loss: 0.08565010875463486.\n",
            "Epoch: 742. Loss: 0.1431882381439209.\n",
            "Epoch: 743. Loss: 0.06420903652906418.\n",
            "Epoch: 744. Loss: 0.1637413650751114.\n",
            "Epoch: 745. Loss: 0.3511073589324951.\n",
            "Epoch: 746. Loss: 0.1914878636598587.\n",
            "Epoch: 747. Loss: 0.11391973495483398.\n",
            "Epoch: 748. Loss: 0.2850967347621918.\n",
            "Epoch: 749. Loss: 0.5298236608505249.\n",
            "Epoch: 750. Loss: 0.15078336000442505.\n",
            "Epoch: 751. Loss: 0.15595503151416779.\n",
            "Epoch: 752. Loss: 0.08506286889314651.\n",
            "Epoch: 753. Loss: 0.1140236109495163.\n",
            "Epoch: 754. Loss: 0.12167644500732422.\n",
            "Epoch: 755. Loss: 0.16535261273384094.\n",
            "Epoch: 756. Loss: 0.2928902804851532.\n",
            "Epoch: 757. Loss: 0.06398114562034607.\n",
            "Epoch: 758. Loss: 0.14519010484218597.\n",
            "Epoch: 759. Loss: 0.05791231617331505.\n",
            "Epoch: 760. Loss: 0.102422334253788.\n",
            "Epoch: 761. Loss: 0.15421079099178314.\n",
            "Epoch: 762. Loss: 0.1196644976735115.\n",
            "Epoch: 763. Loss: 0.26302221417427063.\n",
            "Epoch: 764. Loss: 0.18461881577968597.\n",
            "Epoch: 765. Loss: 0.08603490889072418.\n",
            "Epoch: 766. Loss: 0.21596425771713257.\n",
            "Epoch: 767. Loss: 0.17932958900928497.\n",
            "Epoch: 768. Loss: 0.16340957581996918.\n",
            "Epoch: 769. Loss: 0.11371974647045135.\n",
            "Epoch: 770. Loss: 0.14602607488632202.\n",
            "Epoch: 771. Loss: 0.07484687119722366.\n",
            "Epoch: 772. Loss: 0.19319376349449158.\n",
            "Epoch: 773. Loss: 0.08580847829580307.\n",
            "Epoch: 774. Loss: 0.18527843058109283.\n",
            "Epoch: 775. Loss: 0.12132848054170609.\n",
            "Epoch: 776. Loss: 0.06641249358654022.\n",
            "Epoch: 777. Loss: 0.1250510811805725.\n",
            "Epoch: 778. Loss: 0.13045306503772736.\n",
            "Epoch: 779. Loss: 0.08805640041828156.\n",
            "Epoch: 780. Loss: 0.08888677507638931.\n",
            "Epoch: 781. Loss: 0.17682047188282013.\n",
            "Epoch: 782. Loss: 0.15537096560001373.\n",
            "Epoch: 783. Loss: 0.042279597371816635.\n",
            "Epoch: 784. Loss: 0.14398139715194702.\n",
            "Epoch: 785. Loss: 0.16865770518779755.\n",
            "Epoch: 786. Loss: 0.3073887526988983.\n",
            "Epoch: 787. Loss: 0.09943022578954697.\n",
            "Epoch: 788. Loss: 0.08205196261405945.\n",
            "Epoch: 789. Loss: 0.29333797097206116.\n",
            "Epoch: 790. Loss: 0.12616856396198273.\n",
            "Epoch: 791. Loss: 0.11844870448112488.\n",
            "Epoch: 792. Loss: 0.1272457093000412.\n",
            "Epoch: 793. Loss: 0.06978316605091095.\n",
            "Epoch: 794. Loss: 0.07715987414121628.\n",
            "Epoch: 795. Loss: 0.12278758734464645.\n",
            "Epoch: 796. Loss: 0.06593308597803116.\n",
            "Epoch: 797. Loss: 0.05954381823539734.\n",
            "Epoch: 798. Loss: 0.133004829287529.\n",
            "Epoch: 799. Loss: 0.22639453411102295.\n",
            "Epoch: 800. Loss: 0.21591717004776.\n",
            "Epoch: 801. Loss: 0.08109697699546814.\n",
            "Epoch: 802. Loss: 0.02863193489611149.\n",
            "Epoch: 803. Loss: 0.298035204410553.\n",
            "Epoch: 804. Loss: 0.16377238929271698.\n",
            "Epoch: 805. Loss: 0.17224645614624023.\n",
            "Epoch: 806. Loss: 0.34349074959754944.\n",
            "Epoch: 807. Loss: 0.15408703684806824.\n",
            "Epoch: 808. Loss: 0.12116464972496033.\n",
            "Epoch: 809. Loss: 0.030543841421604156.\n",
            "Epoch: 810. Loss: 0.19609741866588593.\n",
            "Epoch: 811. Loss: 0.15435868501663208.\n",
            "Epoch: 812. Loss: 0.17743395268917084.\n",
            "Epoch: 813. Loss: 0.10716822743415833.\n",
            "Epoch: 814. Loss: 0.21701864898204803.\n",
            "Epoch: 815. Loss: 0.1790686398744583.\n",
            "Epoch: 816. Loss: 0.1725435107946396.\n",
            "Epoch: 817. Loss: 0.12397339940071106.\n",
            "Epoch: 818. Loss: 0.15628448128700256.\n",
            "Epoch: 819. Loss: 0.2305740863084793.\n",
            "Epoch: 820. Loss: 0.20851896703243256.\n",
            "Epoch: 821. Loss: 0.04684770107269287.\n",
            "Epoch: 822. Loss: 0.09972694516181946.\n",
            "Epoch: 823. Loss: 0.2841145396232605.\n",
            "Epoch: 824. Loss: 0.22384582459926605.\n",
            "Epoch: 825. Loss: 0.13170428574085236.\n",
            "Epoch: 826. Loss: 0.05396481603384018.\n",
            "Epoch: 827. Loss: 0.09234558045864105.\n",
            "Epoch: 828. Loss: 0.07175347954034805.\n",
            "Epoch: 829. Loss: 0.10504050552845001.\n",
            "Epoch: 830. Loss: 0.21077975630760193.\n",
            "Epoch: 831. Loss: 0.17981992661952972.\n",
            "Epoch: 832. Loss: 0.07061809301376343.\n",
            "Epoch: 833. Loss: 0.17363043129444122.\n",
            "Epoch: 834. Loss: 0.27052316069602966.\n",
            "Epoch: 835. Loss: 0.2411317080259323.\n",
            "Epoch: 836. Loss: 0.07523100078105927.\n",
            "Epoch: 837. Loss: 0.09993860870599747.\n",
            "Epoch: 838. Loss: 0.20608866214752197.\n",
            "Epoch: 839. Loss: 0.05348256602883339.\n",
            "Epoch: 840. Loss: 0.22867313027381897.\n",
            "Epoch: 841. Loss: 0.14802052080631256.\n",
            "Epoch: 842. Loss: 0.07516822963953018.\n",
            "Epoch: 843. Loss: 0.0988084003329277.\n",
            "Epoch: 844. Loss: 0.09108324348926544.\n",
            "Epoch: 845. Loss: 0.3849576711654663.\n",
            "Epoch: 846. Loss: 0.134248748421669.\n",
            "Epoch: 847. Loss: 0.1663769781589508.\n",
            "Epoch: 848. Loss: 0.1890098601579666.\n",
            "Epoch: 849. Loss: 0.11060433089733124.\n",
            "Epoch: 850. Loss: 0.3123217821121216.\n",
            "Epoch: 851. Loss: 0.0829654112458229.\n",
            "Epoch: 852. Loss: 0.062050994485616684.\n",
            "Epoch: 853. Loss: 0.2010522037744522.\n",
            "Epoch: 854. Loss: 0.061579469591379166.\n",
            "Epoch: 855. Loss: 0.10074091702699661.\n",
            "Epoch: 856. Loss: 0.20134654641151428.\n",
            "Epoch: 857. Loss: 0.10475525259971619.\n",
            "Epoch: 858. Loss: 0.0959257110953331.\n",
            "Epoch: 859. Loss: 0.41192153096199036.\n",
            "Epoch: 860. Loss: 0.1679682582616806.\n",
            "Epoch: 861. Loss: 0.05453971400856972.\n",
            "Epoch: 862. Loss: 0.03448422998189926.\n",
            "Epoch: 863. Loss: 0.07313796877861023.\n",
            "Epoch: 864. Loss: 0.14535121619701385.\n",
            "Epoch: 865. Loss: 0.1527905911207199.\n",
            "Epoch: 866. Loss: 0.20257240533828735.\n",
            "Epoch: 867. Loss: 0.018613159656524658.\n",
            "Epoch: 868. Loss: 0.10659687221050262.\n",
            "Epoch: 869. Loss: 0.12146227061748505.\n",
            "Epoch: 870. Loss: 0.19101007282733917.\n",
            "Epoch: 871. Loss: 0.15537510812282562.\n",
            "Epoch: 872. Loss: 0.19997373223304749.\n",
            "Epoch: 873. Loss: 0.025822147727012634.\n",
            "Epoch: 874. Loss: 0.13414685428142548.\n",
            "Epoch: 875. Loss: 0.12393873184919357.\n",
            "Epoch: 876. Loss: 0.17182472348213196.\n",
            "Epoch: 877. Loss: 0.19609642028808594.\n",
            "Epoch: 878. Loss: 0.09552475064992905.\n",
            "Epoch: 879. Loss: 0.17596672475337982.\n",
            "Epoch: 880. Loss: 0.08018265664577484.\n",
            "Epoch: 881. Loss: 0.1444901078939438.\n",
            "Epoch: 882. Loss: 0.12721647322177887.\n",
            "Epoch: 883. Loss: 0.09406716376543045.\n",
            "Epoch: 884. Loss: 0.37521684169769287.\n",
            "Epoch: 885. Loss: 0.21680264174938202.\n",
            "Epoch: 886. Loss: 0.1097002699971199.\n",
            "Epoch: 887. Loss: 0.18971294164657593.\n",
            "Epoch: 888. Loss: 0.21072611212730408.\n",
            "Epoch: 889. Loss: 0.24853728711605072.\n",
            "Epoch: 890. Loss: 0.6394607424736023.\n",
            "Epoch: 891. Loss: 0.056259654462337494.\n",
            "Epoch: 892. Loss: 0.21955189108848572.\n",
            "Epoch: 893. Loss: 0.14281544089317322.\n",
            "Epoch: 894. Loss: 0.14915353059768677.\n",
            "Epoch: 895. Loss: 0.044680140912532806.\n",
            "Epoch: 896. Loss: 0.16599959135055542.\n",
            "Epoch: 897. Loss: 0.17618083953857422.\n",
            "Epoch: 898. Loss: 0.1376604586839676.\n",
            "Epoch: 899. Loss: 0.1040431559085846.\n",
            "Epoch: 900. Loss: 0.29371359944343567.\n",
            "Epoch: 901. Loss: 0.14691253006458282.\n",
            "Epoch: 902. Loss: 0.10173020511865616.\n",
            "Epoch: 903. Loss: 0.05598875507712364.\n",
            "Epoch: 904. Loss: 0.14415830373764038.\n",
            "Epoch: 905. Loss: 0.15324586629867554.\n",
            "Epoch: 906. Loss: 0.12913240492343903.\n",
            "Epoch: 907. Loss: 0.06921070069074631.\n",
            "Epoch: 908. Loss: 0.06608566641807556.\n",
            "Epoch: 909. Loss: 0.27597206830978394.\n",
            "Epoch: 910. Loss: 0.1472729742527008.\n",
            "Epoch: 911. Loss: 0.10499381273984909.\n",
            "Epoch: 912. Loss: 0.2384108155965805.\n",
            "Epoch: 913. Loss: 0.1507827788591385.\n",
            "Epoch: 914. Loss: 0.09976918995380402.\n",
            "Epoch: 915. Loss: 0.12582191824913025.\n",
            "Epoch: 916. Loss: 0.17056865990161896.\n",
            "Epoch: 917. Loss: 0.1653420627117157.\n",
            "Epoch: 918. Loss: 0.2319539189338684.\n",
            "Epoch: 919. Loss: 0.17407171428203583.\n",
            "Epoch: 920. Loss: 0.11061742901802063.\n",
            "Epoch: 921. Loss: 0.10604844242334366.\n",
            "Epoch: 922. Loss: 0.140217587351799.\n",
            "Epoch: 923. Loss: 0.26177969574928284.\n",
            "Epoch: 924. Loss: 0.12799277901649475.\n",
            "Epoch: 925. Loss: 0.09620187431573868.\n",
            "Epoch: 926. Loss: 0.14265701174736023.\n",
            "Epoch: 927. Loss: 0.2002834528684616.\n",
            "Epoch: 928. Loss: 0.14780686795711517.\n",
            "Epoch: 929. Loss: 0.26800838112831116.\n",
            "Epoch: 930. Loss: 0.20663361251354218.\n",
            "Epoch: 931. Loss: 0.08681724220514297.\n",
            "Epoch: 932. Loss: 0.14829860627651215.\n",
            "Epoch: 933. Loss: 0.13421180844306946.\n",
            "Epoch: 934. Loss: 0.03711935132741928.\n",
            "Epoch: 935. Loss: 0.12869566679000854.\n",
            "Epoch: 936. Loss: 0.14626231789588928.\n",
            "Epoch: 937. Loss: 0.21227489411830902.\n",
            "Epoch: 938. Loss: 0.11115819215774536.\n",
            "Epoch: 939. Loss: 0.13808466494083405.\n",
            "Epoch: 940. Loss: 0.27221888303756714.\n",
            "Epoch: 941. Loss: 0.19299623370170593.\n",
            "Epoch: 942. Loss: 0.10599963366985321.\n",
            "Epoch: 943. Loss: 0.13999849557876587.\n",
            "Epoch: 944. Loss: 0.13471440970897675.\n",
            "Epoch: 945. Loss: 0.14452698826789856.\n",
            "Epoch: 946. Loss: 0.028395986184477806.\n",
            "Epoch: 947. Loss: 0.06978528946638107.\n",
            "Epoch: 948. Loss: 0.15108874440193176.\n",
            "Epoch: 949. Loss: 0.20972667634487152.\n",
            "Epoch: 950. Loss: 0.14811833202838898.\n",
            "Epoch: 951. Loss: 0.1670696884393692.\n",
            "Epoch: 952. Loss: 0.09066810458898544.\n",
            "Epoch: 953. Loss: 0.10522700101137161.\n",
            "Epoch: 954. Loss: 0.06870853155851364.\n",
            "Epoch: 955. Loss: 0.10213655233383179.\n",
            "Epoch: 956. Loss: 0.1989336609840393.\n",
            "Epoch: 957. Loss: 0.0643264502286911.\n",
            "Epoch: 958. Loss: 0.24842073023319244.\n",
            "Epoch: 959. Loss: 0.1643156111240387.\n",
            "Epoch: 960. Loss: 0.19327951967716217.\n",
            "Epoch: 961. Loss: 0.3748042583465576.\n",
            "Epoch: 962. Loss: 0.082671158015728.\n",
            "Epoch: 963. Loss: 0.09960155189037323.\n",
            "Epoch: 964. Loss: 0.129048153758049.\n",
            "Epoch: 965. Loss: 0.04736395180225372.\n",
            "Epoch: 966. Loss: 0.027203712612390518.\n",
            "Epoch: 967. Loss: 0.1689058095216751.\n",
            "Epoch: 968. Loss: 0.2315790057182312.\n",
            "Epoch: 969. Loss: 0.09375191479921341.\n",
            "Epoch: 970. Loss: 0.03699596971273422.\n",
            "Epoch: 971. Loss: 0.3309912085533142.\n",
            "Epoch: 972. Loss: 0.09931832551956177.\n",
            "Epoch: 973. Loss: 0.06475118547677994.\n",
            "Epoch: 974. Loss: 0.11373874545097351.\n",
            "Epoch: 975. Loss: 0.1598210632801056.\n",
            "Epoch: 976. Loss: 0.08834812045097351.\n",
            "Epoch: 977. Loss: 0.22659391164779663.\n",
            "Epoch: 978. Loss: 0.08956205099821091.\n",
            "Epoch: 979. Loss: 0.06603333353996277.\n",
            "Epoch: 980. Loss: 0.06140248849987984.\n",
            "Epoch: 981. Loss: 0.1689058095216751.\n",
            "Epoch: 982. Loss: 0.2093087136745453.\n",
            "Epoch: 983. Loss: 0.22261695563793182.\n",
            "Epoch: 984. Loss: 0.03411446884274483.\n",
            "Epoch: 985. Loss: 0.10290608555078506.\n",
            "Epoch: 986. Loss: 0.12918606400489807.\n",
            "Epoch: 987. Loss: 0.11076222360134125.\n",
            "Epoch: 988. Loss: 0.18734687566757202.\n",
            "Epoch: 989. Loss: 0.2615709602832794.\n",
            "Epoch: 990. Loss: 0.12362758815288544.\n",
            "Epoch: 991. Loss: 0.16287760436534882.\n",
            "Epoch: 992. Loss: 0.15471825003623962.\n",
            "Epoch: 993. Loss: 0.16625797748565674.\n",
            "Epoch: 994. Loss: 0.10881566256284714.\n",
            "Epoch: 995. Loss: 0.0931636393070221.\n",
            "Epoch: 996. Loss: 0.02686711587011814.\n",
            "Epoch: 997. Loss: 0.04438185319304466.\n",
            "Epoch: 998. Loss: 0.14758311212062836.\n",
            "Epoch: 999. Loss: 0.2540512979030609.\n",
            "Epoch: 1000. Loss: 0.036857083439826965.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, \": \", param.data)\n",
        "# theta_0 = bias, theta_1:2 = weight\n",
        "\n",
        "# Interpretation:\n",
        "# linear1.weight :  tensor([[3.4394, 8.9426]]) \n",
        "# -> both parameters are positively correlated, i.e. if any of them increase, \n",
        "# the probability of having class \"1\" increases\n",
        "# linear1.bias :  tensor([-31.8393])\n",
        "# -> offset prediction by -31.8, which conteracts the large positive weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL3me960jWNv",
        "outputId": "6ef9149f-486e-4eee-d8ef-424a59a09961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear1.weight :  tensor([[3.4982, 8.9113]])\n",
            "linear1.bias :  tensor([-31.5534])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(input_columns[:,0].min(), input_columns[:,0].max(), 10)\n",
        "y = np.linspace(input_columns[:,1].min(), input_columns[:,1].max(), 10)\n",
        "xx, yy = np.meshgrid(x,y)\n",
        "X, Y = xx.flatten(), yy.flatten()\n",
        "XY = np.vstack([X,Y]).T\n",
        "\n",
        "with torch.no_grad():\n",
        "    predicted = model(Variable(torch.from_numpy(XY).type(torch.float32))).data.numpy()\n",
        "\n",
        "predicted = predicted.squeeze()\n",
        "predicted = predicted.reshape(10,10)"
      ],
      "metadata": {
        "id": "Do9ripaBgH3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure(data=[\n",
        "    go.Surface(\n",
        "        contours = {\n",
        "            \"z\": {\"show\": True, \"start\": 0.5, \"end\": 0.5001, \"size\": 0.05}\n",
        "        },\n",
        "        x = x,\n",
        "        y = y,\n",
        "        z = predicted,\n",
        "        opacity=0.5\n",
        "    ),\n",
        "    go.Scatter3d(\n",
        "        x = df[\"petal_length\"],\n",
        "        y = df['petal_width'],\n",
        "        z = df['class_idx'],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            color=df['class_idx'],\n",
        "            opacity=0.99,\n",
        "        )\n",
        "    )\n",
        "])\n",
        "\n",
        "fig.update_layout(margin = dict(l=0, r=0, b=0, t=0),\n",
        "                  scene = {\n",
        "                      \"camera_eye\": {\"x\":7, \"y\":-3., \"z\":0.5},\n",
        "                      \"aspectratio\": {\"x\":8, \"y\":3, \"z\":2}\n",
        "                      }\n",
        "                  )\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "-VqpXUZEUhPW",
        "outputId": "574152e1-535b-4ede-937f-cbfe3b811aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"2ddb8e1c-b26b-4c0c-bd45-4430d3c39fb6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2ddb8e1c-b26b-4c0c-bd45-4430d3c39fb6\")) {                    Plotly.newPlot(                        \"2ddb8e1c-b26b-4c0c-bd45-4430d3c39fb6\",                        [{\"contours\":{\"z\":{\"end\":0.5001,\"show\":true,\"size\":0.05,\"start\":0.5}},\"opacity\":0.5,\"x\":[3.0,3.4333333439297147,3.8666666878594294,4.300000031789144,4.733333375718859,5.166666719648573,5.600000063578287,6.033333407508002,6.466666751437717,6.900000095367432],\"y\":[1.0,1.1666666666666667,1.3333333333333333,1.5,1.6666666666666665,1.8333333333333333,2.0,2.1666666666666665,2.333333333333333,2.5],\"z\":[[5.302234058035538e-06,2.4143490009009838e-05,0.00010992875468218699,0.0005003697006031871,0.002274410566315055,0.010273570194840431,0.04513334110379219,0.1771109402179718,0.49496525526046753,0.8169420957565308],[2.34137933148304e-05,0.00010660682164598256,0.000485253898659721,0.0022058202885091305,0.009966175071895123,0.04382909834384918,0.17268230020999908,0.4872952401638031,0.8123077750205994,0.9517075419425964],[0.0001033852604450658,0.00047059624921530485,0.0021392982453107834,0.009667887352406979,0.042560942471027374,0.16834229230880737,0.4796317517757416,0.8075839877128601,0.9502772688865662,0.9886395931243896],[0.00045637943549081683,0.0020747780799865723,0.009378425776958466,0.041327740997076035,0.16408944129943848,0.47197777032852173,0.8027698993682861,0.9488070607185364,0.9882897138595581,0.9974045157432556],[0.0020121962297707796,0.00909756775945425,0.04012877494096756,0.15992313623428345,0.46433699131011963,0.7978658080101013,0.9472956657409668,0.987929105758667,0.9973238706588745,0.9994111061096191],[0.008825037628412247,0.038963284343481064,0.15584300458431244,0.45671263337135315,0.7928714156150818,0.9457422494888306,0.9875576496124268,0.9972407817840576,0.9993927478790283,0.9998666048049927],[0.03783024102449417,0.15184840559959412,0.4491085112094879,0.7877861261367798,0.944145917892456,0.9871748089790344,0.997154951095581,0.9993738532066345,0.9998624324798584,0.9999698400497437],[0.14793799817562103,0.44152864813804626,0.7826102375984192,0.9425053000450134,0.9867804646492004,0.9970666766166687,0.9993543028831482,0.9998581409454346,0.9999688863754272,0.9999932050704956],[0.43397489190101624,0.77734375,0.9408193826675415,0.9863741397857666,0.9969754219055176,0.9993341565132141,0.9998537302017212,0.9999678134918213,0.9999929666519165,0.9999984502792358],[0.7719869017601013,0.939087450504303,0.9859553575515747,0.9968814849853516,0.999313473701477,0.9998490810394287,0.9999668598175049,0.9999927282333374,0.9999984502792358,0.9999996423721313]],\"type\":\"surface\"},{\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"opacity\":0.99},\"mode\":\"markers\",\"x\":[4.7,4.5,4.9,4.0,4.6,4.5,4.7,3.3,4.6,3.9,3.5,4.2,4.0,4.7,3.6,4.4,4.5,4.1,4.5,3.9,4.8,4.0,4.9,4.7,4.3,4.4,4.8,5.0,4.5,3.5,3.8,3.7,3.9,5.1,4.5,4.5,4.7,4.4,4.1,4.0,4.4,4.6,4.0,3.3,4.2,4.2,4.2,4.3,3.0,4.1,6.0,5.1,5.9,5.6,5.8,6.6,4.5,6.3,5.8,6.1,5.1,5.3,5.5,5.0,5.1,5.3,5.5,6.7,6.9,5.0,5.7,4.9,6.7,4.9,5.7,6.0,4.8,4.9,5.6,5.8,6.1,6.4,5.6,5.1,5.6,6.1,5.6,5.5,4.8,5.4,5.6,5.1,5.1,5.9,5.7,5.2,5.0,5.2,5.4,5.1],\"y\":[1.4,1.5,1.5,1.3,1.5,1.3,1.6,1.0,1.3,1.4,1.0,1.5,1.0,1.4,1.3,1.4,1.5,1.0,1.5,1.1,1.8,1.3,1.5,1.2,1.3,1.4,1.4,1.7,1.5,1.0,1.1,1.0,1.2,1.6,1.5,1.6,1.5,1.3,1.3,1.3,1.2,1.4,1.2,1.0,1.3,1.2,1.3,1.3,1.1,1.3,2.5,1.9,2.1,1.8,2.2,2.1,1.7,1.8,1.8,2.5,2.0,1.9,2.1,2.0,2.4,2.3,1.8,2.2,2.3,1.5,2.3,2.0,2.0,1.8,2.1,1.8,1.8,1.8,2.1,1.6,1.9,2.0,2.2,1.5,1.4,2.3,2.4,1.8,1.8,2.1,2.4,2.3,1.9,2.3,2.5,2.3,1.9,2.0,2.3,1.8],\"z\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"margin\":{\"l\":0,\"r\":0,\"b\":0,\"t\":0},\"scene\":{\"camera\":{\"eye\":{\"x\":7,\"y\":-3.0,\"z\":0.5}},\"aspectratio\":{\"x\":8,\"y\":3,\"z\":2}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2ddb8e1c-b26b-4c0c-bd45-4430d3c39fb6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 Exercise\n",
        "\n",
        "Apply logistic regression to the [MNIST handwritten digits](http://yann.lecun.com/exdb/mnist/) dataset. Main differences to the Iris dataset:\n",
        "- the inputs are images of shape [28,28,1] and need to be flattened out\n",
        "- the output here is not the probability of being in one class (as in the problem we discusses here), but 10 classes and the probability of being in each of them.\n",
        "\n",
        "> Hint: You might find help [here](https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19)."
      ],
      "metadata": {
        "id": "ElFgc-Ys3710"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# TODO\n",
        "\n",
        "\n",
        "####################"
      ],
      "metadata": {
        "id": "p5Y4xCoR37ER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}